{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de70be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Change the current working directory to the Desktop\n",
    "os.chdir(r'C:\\Users\\91990\\Desktop')\n",
    "# Verify the current working directory\n",
    "print(\"Current Directory:\", os.getcwd())\n",
    "\n",
    "\n",
    "\n",
    "!pip install textblob\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_md\n",
    "\n",
    "!pip uninstall numpy -y\n",
    "!pip install numpy==1.23.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329d74aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb3a5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import download\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "download('punkt')\n",
    "download('stopwords')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e453bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Check the current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(f\"Current working directory: {current_directory}\")\n",
    "\n",
    "# **Step : Data Collection**\n",
    "# Load the CSV file into a dataframe\n",
    "df1 = pd.read_csv(os.path.join(current_directory, 'text_data.csv'))  # Load first big file\n",
    "\n",
    "# Concatenate all dataframes (if you have multiple dataframes to concatenate, you can add them here)\n",
    "df = pd.concat([df1, ], ignore_index=True)\n",
    "print(\"Data Shape after loading:\", df.shape)\n",
    "\n",
    "# **Step : Retain only unique values in the 'Text0' column**\n",
    "df = df.drop_duplicates(subset='Text0', keep='first')\n",
    "print(\"Data Shape after removing duplicates:\", df.shape)\n",
    "\n",
    "# **Step : Remove rows where any column is empty**\n",
    "df = df.dropna()  # This removes all rows with any NaN values in any column\n",
    "print(\"Data Shape after removing rows with any empty values:\", df.shape)\n",
    "\n",
    "# **Step : Convert the 'Date' column to datetime format including time**\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%Y %H:%M')  # Adjust the format for date and time\n",
    "\n",
    "# **Step : Create additional columns for 'Date' and 'Time'**\n",
    "df['Date_only'] = df['Date'].dt.date  # Extract the date part\n",
    "df['Time_only'] = df['Date'].dt.time  # Extract the time part\n",
    "\n",
    "# **Step : Filter rows for May 2021 to August 2021**\n",
    "df_may_to_aug = df[(df['Date'] >= '2021-05-01') & (df['Date'] <= '2021-08-31')]\n",
    "\n",
    "# **Step : Filter rows for September 2021 to October 2021**\n",
    "df_sep_oct = df[(df['Date'] >= '2021-09-01') & (df['Date'] <= '2021-10-31')]\n",
    "\n",
    "# Print the first few rows to verify\n",
    "print(df_may_to_aug.shape)\n",
    "print(df_sep_oct.shape)\n",
    "\n",
    "# **Step : Save the filtered DataFrames as CSVs in the current directory**\n",
    "#df_may_to_aug.to_csv(os.path.join(current_directory, 'df_may_to_aug.csv'), index=False)\n",
    "#df_sep_oct.to_csv(os.path.join(current_directory, 'df_sep_oct.csv'), index=False)\n",
    "\n",
    "# **Step : Print the first row and shape of the DataFrame after selection**\n",
    "print(df_may_to_aug.head(1))\n",
    "print(f\"Shape of selected rows: {df_may_to_aug.shape}\")\n",
    "\n",
    "# **Step : Update df with the selected rows**\n",
    "df = df_may_to_aug\n",
    "\n",
    "# **Step : Check if there are any null values left in any columns**\n",
    "null_values = df.isnull().sum()\n",
    "print(\"\\nNull values in each column after processing:\")\n",
    "print(null_values)\n",
    "\n",
    "# If any column has null values, display a message\n",
    "if null_values.any():\n",
    "    print(\"\\nThere are still null values in some columns.\")\n",
    "else:\n",
    "    print(\"\\nNo null values found in any column.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cb9eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "slang_lexicon = {\n",
    "    \"feel like shit\": [\n",
    "        \"fatigue\", \"muscle aches\", \"general malaise\", \"feeling rundown\", \"exhaustion\", \"lack of energy\", \"overall weakness\",\n",
    "        \"unwell\", \"drained\", \"debilitated\", \"feeling awful\", \"utterly fatigued\", \"feeling terrible\"\n",
    "    ],\n",
    "    \"the vaccine is a shield against the storm\": [\n",
    "        \"protection\", \"immunization\", \"disease prevention\"\n",
    "    ],\n",
    "    \"i feel like i’ve been run over by a truck\": [\n",
    "        \"muscle sore\", \"fatigue\", \"body ache\"\n",
    "    ],\n",
    "    \"the covid-19 crisis is spreading like wildfire\": [\n",
    "        \"rapid spread\", \"widespread transmission\", \"accelerated infection rate\"\n",
    "    ],\n",
    "    \"i’ve seen a million posts about vaccine side effects\": [\n",
    "        \"large volume of posts\", \"information overload\", \"overwhelming number of reports\"\n",
    "    ],\n",
    "    \"the virus is playing hide and seek\": [\n",
    "        \"virus evasion\", \"difficult to track\"\n",
    "    ],\n",
    "    \"the vaccine is our hero in this fight\": [\n",
    "        \"vaccine effectiveness\", \"heroic response\"\n",
    "    ],\n",
    "    \"nothing like waiting in line for hours to get the vaccine that will save us all\": [\n",
    "        \"long wait times\", \"vaccine access delay\", \"inconvenience of vaccination process\"\n",
    "    ],\n",
    "    \"vaccinating the world is like building the great wall of china\": [\n",
    "        \"large-scale vaccination effort\", \"extensive public health campaign\"\n",
    "    ],\n",
    "    \"yeah, the vaccine’s side effects are just a little inconvenient\": [\n",
    "        \"mild side effects\", \"tolerable discomfort\", \"temporary symptoms\"\n",
    "    ],\n",
    "    \"let’s vaccine and chill\": [\n",
    "        \"call to vaccinate\", \"relaxation after vaccination\"\n",
    "    ],\n",
    "    \"the vaccine distribution was an organized chaos\": [\n",
    "        \"coordinated but challenging distribution\", \"efficient yet overwhelmed system\"\n",
    "    ],\n",
    "    \"we’re in the homestretch now, just waiting on the vaccine\": [\n",
    "        \"final stages of vaccine rollout\", \"awaiting vaccine availability\"\n",
    "    ],\n",
    "    \"drained\": [\n",
    "        \"exhausted\", \"wiped out\", \"hit by a truck\", \"wrecked\", \"feeling rundown\", \"heavy\", \"sluggish\", \"foggy\", \"dizzy\", \"sore\"\n",
    "    ],\n",
    "    \"weary\": [\n",
    "        \"fatigue\", \"drained\", \"worn out\", \"sleep-deprived\", \"exhausted\", \"tired\", \"depleted\", \"low energy\", \"sluggish\", \"lethargic\"\n",
    "    ],\n",
    "    \"feeling flushed\": [\n",
    "        \"feeling hot\", \"feeling feverish\", \"overheated\"\n",
    "    ],\n",
    "    \"feeling blue\": [\n",
    "        \"feeling emotionally low\", \"feeling down\", \"mood swings\"\n",
    "    ],\n",
    "    \"feeling under the weather\": [\n",
    "        \"feeling sick\", \"feeling unwell\", \"general malaise\"\n",
    "    ],\n",
    "    \"feeling spacey\": [\n",
    "        \"dizziness\", \"lightheadedness\", \"giddiness\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "known_symptoms = [\n",
    "    # Physical Symptoms (Direct)\n",
    "    'fatigue', 'fever', 'headache', 'muscle aches', 'sore arm', 'chills', 'nausea', 'dizziness',\n",
    "    'shortness of breath', 'drowsiness', 'sweating', 'stomach cramps', 'joint pain', 'back pain',\n",
    "    'body aches', 'weakness', 'sore throat', 'feeling feverish', 'brain fog', 'migraines',\n",
    "    'shivering', 'muscle stiffness', 'redness at injection site',\n",
    "    # Emotional/Mood Symptoms\n",
    "    'mood swings', 'irritability', 'feeling down', 'emotional instability', 'feeling emotionally low',\n",
    "    'feeling overwhelmed', 'feeling anxious', 'feeling on edge', 'feeling blue', 'feeling out of sorts',\n",
    "    'mood fluctuations',\n",
    "    # Sleep-Related Symptoms\n",
    "    'sleepiness', 'excessive sleepiness', 'tiredness', 'trouble sleeping', 'sleep-deprived', 'feeling drowsy',\n",
    "    # Abdominal/Gastrointestinal Symptoms\n",
    "    'upset stomach', 'bloating', 'gassy', 'intestinal discomfort', 'stomach discomfort', 'gastritis', 'heartburn',\n",
    "    'abdominal cramps', 'gastrointestinal distress', 'stomach queasiness',\n",
    "    # Injection-Specific Symptoms\n",
    "    'arm soreness', 'injection site pain', 'pain at injection site', 'muscle tightness', 'inflammation at injection site',\n",
    "    'redness at injection site', 'tender arm', 'deltoid pain', 'injection site swelling',\n",
    "    # Other/General Symptoms\n",
    "    'feeling hot', 'sensitive scalp', 'neck tension', 'dehydration', 'feeling unwell', 'feeling sick',\n",
    "    'giddiness', 'dry mouth', 'hunger', 'uncontrollable shaking', 'lightheaded',\n",
    "    'head pressure', 'neck stiffness', 'sweaty palms',\n",
    "    # Extreme Symptoms (Severe or Unexpected)\n",
    "    'feeling like passing out', 'feeling faint', 'feeling incapacitated', 'feeling like collapsing',\n",
    "    'feeling weak to the point of immobility', 'difficulty breathing', 'unable to move',\n",
    "    'extreme fatigue', 'extreme tiredness', 'muscle spasms'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "uncommon_vaccine_side_effects_lexicon = [\n",
    "    \"myocarditis\", \"thrombosis\", \"blood clots\", \"anaphylaxis\", \"heart inflammation\",\n",
    "    \"vaccine-induced immune response\", \"neurological disorders\", \"autoimmune disease\",\n",
    "    \"severe headache\", \"persistent fever\", \"unusual bleeding\", \"rash\", \"hives\",\n",
    "    \"blood pressure\", \"paralysis\", \"dizziness\", \"shortness of breath\", \"arrhythmia\",\n",
    "    \"nausea\", \"fatigue\", \"chest pain\", \"pericarditis\", \"guilain-barre syndrome\",\n",
    "    \"shingles\", \"stroke\", \"hemorrhagic fever\", \"myositis\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "time_patterns = [\n",
    "    # Patterns for Analysis related to Vaccines\n",
    "    r\"(not\\s*get\\s*vaccine)\",\n",
    "    r\"(not\\s*believe\\s*in\\s*vaccine)\",\n",
    "    r\"(vaccine\\s*(has\\s*no\\s*side\\s*effects|was\\s*effective))\",\n",
    "    r\"(vaccine\\s*(should\\s*be\\s*mandatory|should\\s*be\\s*choice))\",\n",
    "    r\"(i\\s*(am\\s*scared|am\\s*hesitant)\\s*about\\s*vaccine)\",\n",
    "    r\"(vaccine\\s*does\\s*not\\s*prevent\\s*covid)\",\n",
    "    r\"(vaccine\\s*gave\\s*me\\s*side\\s*effects)\",\n",
    "    r\"(vaccine\\s*is\\s*a\\s*choice\\s*not\\s*a\\s*mandate)\",\n",
    "    r\"(vaccine\\s*(is\\s*helping|has\\s*been\\s*effective))\",\n",
    "    r\"(vaccine\\s*has\\s*caused\\s*issues|vaccine\\s*causes\\s*harm)\",\n",
    "    r\"(vaccine\\s*is\\s*safe|vaccine\\s*has\\s*worked\\s*for\\s*me)\",\n",
    "    r\"(vaccine\\s*(does\\s*not\\s*work|does\\s*not\\s*prevent\\s*covid))\",\n",
    "    r\"(not\\s*going\\s*to\\s*take\\s*vaccine)\",\n",
    "    r\"(vaccine\\s*has\\s*side\\s*effects\\s*(but\\s*still\\s*taking\\s*it|had\\s*no\\s*problem))\",\n",
    "    r\"(do\\s*not\\s*want\\s*the\\s*vaccine)\",\n",
    "    r\"(vaccinated\\s*and\\s*still\\s*get\\s*covid)\",\n",
    "    r\"(side\\s*effects\\s*from\\s*vaccine)\",\n",
    "    r\"(i\\s*feel\\s*confident\\s*after\\s*vaccine)\",\n",
    "    r\"(it\\s*worked\\s*for\\s*me\\s*but\\s*not\\s*for\\s*everyone)\",\n",
    "    r\"(still\\s*believe\\s*vaccine\\s*works)\",\n",
    "    r\"(hesitant\\s*about\\s*vaccine\\s*but\\s*getting\\s*it)\",\n",
    "    r\"(vaccine\\s*should\\s*be\\s*voluntary)\",\n",
    "    r\"(vaccine\\s*does\\s*not\\s*prevent\\s*certain\\s*strains)\",\n",
    "    r\"(there\\s*are\\s*issues\\s*with\\s*vaccine)\",\n",
    "    r\"(vaccine\\s*is\\s*a\\s*choice\\s*for\\s*individuals)\",\n",
    "\n",
    "    # Alert keywords added to the time_patterns lexicon\n",
    "    r\"(threat)\",  # Alert term: life-threatening\n",
    "    r\"(emergency)\",  # Alert term: emergency\n",
    "    r\"(ICU)\",  # Alert term: ICU\n",
    "    r\"(death)\",  # Alert term: death\n",
    "\n",
    "    # Mandate and Forced Vaccine (Mandate is forced and unwanted)\n",
    "    r\"((vaccine\\s*mandates\\s*(are|should\\s*be)\\s*(necessary|unnecessary|unconstitutional|forced|wrong)|vaccine\\s*passport\\s*(is|should\\s*be)\\s*(illegal|unethical|infringing|wrong)))\",\n",
    "\n",
    "    # Negation of Vaccine Mandates\n",
    "    r\"(\\b(not\\s*enough\\s*vaccine\\s*availability|vaccine\\s*mandates\\s*should\\s*not\\s*be\\s*forced|vaccine\\s*mandates\\s*should\\s*be\\s*optional))\",\n",
    "\n",
    "    # Personal choice & free will\n",
    "    r\"((vaccine\\s*is\\s*personal\\s*choice|vaccine\\s*(should\\s*not\\s*be)\\s*forced|vaccine\\s*is\\s*optional))\",\n",
    "\n",
    "    # Positive (Belief in Science/Trust in Vaccine)\n",
    "    r\"(vaccine\\s*is\\s*backed\\s*by\\s*science|vaccine\\s*reduces\\s*infection\\s*risk|vaccine\\s*(protects|saves)\\s*lives|vaccine\\s*is\\s*scientifically\\s*proven)\",\n",
    "\n",
    "    # Negative (Skepticism about Science/Belief)\n",
    "    r\"(vaccine\\s*is\\s*unproven|vaccine\\s*is\\s*unsafe|vaccine\\s*is\\s*experimental|vaccine\\s*is\\s*just\\s*political)\",\n",
    "\n",
    "    # terms where \"not\" negates positive claims (e.g., negating vaccine benefits)\n",
    "    r\"(\\bnot\\s*effective\\s*against\\s*covid|vaccine\\s*does\\s*not\\s*prevent\\s*covid|vaccine\\s*does\\s*not\\s*reduce\\s*spread)\",\n",
    "\n",
    "    # Mixed terms (Vaccination + Side effects)\n",
    "    r\"(vaccine\\s*has\\s*side\\s*effects\\s*(but\\s*is\\s*still\\s*necessary|and\\s*is\\s*worth\\s*the\\s*risks|and\\s*is\\s*minor\\s*compared\\s*to\\s*disease))\",\n",
    "\n",
    "    # Terms in Relation to Specific Groups (vaccine is important for some people, not for others)\n",
    "    r\"(vaccine\\s*is\\s*important\\s*for\\s*(health\\s*care\\s*workers|elderly|those\\s*with\\s*pre-existing\\s*conditions|high\\s*risk\\s*populations))\",\n",
    "\n",
    "    # Terms on Covid & Vaccine Benefits\n",
    "    r\"(covid\\s*is\\s*real\\s*and\\s*vaccine\\s*is\\s*important|covid\\s*should\\s*not\\s*be\\s*taken\\s*lightly|covid\\s*can\\s*kill\\s*and\\s*vaccine\\s*helps)\",\n",
    "    r\"(covid\\s*is\\s*serious|covid\\s*can\\s*be\\sdangerous|covid\\s*has\\s*changed\\s*the\\s*world|covid\\s*is\\s*a\\s*threat)\",\n",
    "\n",
    "    # Patterns related to concerns and skepticism but open to vaccination\n",
    "    r\"(concerned\\s*about\\s*vaccine\\s*but\\s*still\\s*getting\\s*it)\",\n",
    "    r\"(vaccine\\s*has\\s*risks\\s*but\\s*is\\s*necessary)\",\n",
    "    r\"(unsure\\s*about\\s*vaccine\\s*but\\s*still\\s*considering\\s*it)\",\n",
    "\n",
    "    # Positive term with no negation\n",
    "    r\"(?i)(HOORAY\\s*FOR\\s*BEING|feel\\s*good|feel\\s*great|thanks\\s*for\\s*the\\s*compliment|good\\s*day|thank\\s*you|love|works\\s*for\\s*me|exited|built\\s*different|feel\\s*like\\s*doing|positive\\s*effect)\",\n",
    "\n",
    "    # Negative term Patterns\n",
    "    r\"(?i)(won't\\s*work|isn't\\s*needed|don't\\s*think|can't\\s*believe|never\\s*going\\s*back|no\\s*way\\s*to\\s*prove|does\\s*not\\s*work|not\\s*safe|bad\\s*side\\s*effects|problem\\s*with\\s*vaccine|not\\s*effective|failure\\s*of)\",\n",
    "\n",
    "    # Patterns for negation\n",
    "    r\"(?i)(not\\s*work|not\\s*happy|not\\s*good|can't\\s*be|never\\s*needed|don't\\s*agree|can't\\s*accept|didn't\\s*feel|won't\\s*agree|don't\\s*like|not\\s*comfortable)\",\n",
    "    r\"(?i)(no\\s*problem|no\\s*side\\s*effects|no\\s*issues|won't\\s*affect|not\\s*concerned|didn't\\s*mind|don't\\s*care|not\\s*worried)\",\n",
    "\n",
    "    # Detect negation flipping positive term\n",
    "    r\"(?i)\\b(not|never|n't|can't|won't)\\b.*\\b(positive|great|safe|effective|good|happy)\\b\",\n",
    "    r\"(?i)\\b(not|never|n't|can't|won't)\\b.*\\b(works|help|successful)\\b\",\n",
    "\n",
    "    # Detect negation flipping negative term\n",
    "    r\"(?i)\\b(not|never|n't|can't|won't)\\b.*\\b(bad|problem|failure|wrong|negative)\\b\",\n",
    "    r\"(?i)\\b(not|never|n't|can't|won't)\\b.*\\b(worst|issue|failed|sucks|hate)\\b\",\n",
    "\n",
    "    # Time patterns for days, hours, minutes before\n",
    "    r\"(\\d+\\s*(day|days|hour|hours|minute|minutes)\\s*before)\",  # E.g., 3 days before, 5 hours before\n",
    "    r\"(in\\s*(\\d+\\s*(day|days|hour|hours|minute|minutes))\\s*before)\",  # E.g., in 3 days before\n",
    "    r\"(just\\s*before\\s*(\\d+\\s*(day|days|hour|hours|minute|minutes)))\",  # E.g., just before 2 hours\n",
    "    r\"(before\\s*(\\d+\\s*(day|days|hour|hours|minute|minutes)))\",  # E.g., before 1 minute\n",
    "    r\"(\\d+\\s*(second|seconds)\\s*ago)\",  # E.g., 10 seconds ago\n",
    "    r\"(\\d+\\s*(minute|minutes)\\s*ago)\",  # E.g., 5 minutes ago\n",
    "    r\"(\\d+\\s*(hour|hours)\\s*ago)\",  # E.g., 2 hours ago\n",
    "    r\"(\\d+\\s*(day|days)\\s*ago)\",  # E.g., 3 days ago\n",
    "    r\"(\\d+\\s*(week|weeks)\\s*ago)\",  # E.g., 1 week ago\n",
    "    r\"(just\\s*now)\",  # Just now\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define a human-readable label for each time pattern\n",
    "time_pattern_labels = {\n",
    "    r\"(not\\s*get\\s*vaccine)\": \"Negative Term\",\n",
    "    r\"(not\\s*believe\\s*in\\s*vaccine)\": \"Negative Term\",\n",
    "    r\"(vaccine\\s*(has\\s*no\\s*side\\s*effects|was\\s*effective))\": \"Positive Term\",\n",
    "    r\"(vaccine\\s*(should\\s*be\\s*mandatory|should\\s*be\\s*choice))\": \"Mandate & Choice\",\n",
    "    r\"(i\\s*(am\\s*scared|am\\s*hesitant)\\s*about\\s*vaccine)\": \"Hesitation/Side Effects\",\n",
    "    r\"(vaccine\\s*does\\s*not\\s*prevent\\s*covid)\": \"Negative Term\",\n",
    "    r\"(vaccine\\s*gave\\s*me\\s*side\\s*effects)\": \"Hesitation/Side Effects\",\n",
    "    r\"(vaccine\\s*is\\s*a\\s*choice\\s*not\\s*a\\s*mandate)\": \"Choice & Mandates\",\n",
    "    r\"(vaccine\\s*(is\\s*helping|has\\s*been\\s*effective))\": \"Positive Term\",\n",
    "    r\"(vaccine\\s*has\\s*caused\\s*issues|vaccine\\s*causes\\s*harm)\": \"Negative Term\",\n",
    "    r\"(vaccine\\s*is\\s*safe|vaccine\\s*has\\s*worked\\s*for\\s*me)\": \"Positive Term\",\n",
    "    r\"(vaccine\\s*(does\\s*not\\s*work|does\\s*not\\s*prevent\\s*covid))\": \"Negative Term\",\n",
    "    r\"(not\\s*going\\s*to\\s*take\\s*vaccine)\": \"Negative Term\",\n",
    "    r\"(vaccine\\s*has\\s*side\\s*effects\\s*(but\\s*still\\s*taking\\s*it|had\\s*no\\s*problem))\": \"Hesitation/Side Effects\",\n",
    "    r\"(do\\s*not\\s*want\\s*the\\s*vaccine)\": \"Negative Term\",\n",
    "    r\"(vaccinated\\s*and\\s*still\\s*get\\s*covid)\": \"Negative Term\",\n",
    "    r\"(side\\s*effects\\s*from\\s*vaccine)\": \"Hesitation/Side Effects\",\n",
    "    r\"(i\\s*feel\\s*confident\\s*after\\s*vaccine)\": \"Positive Term\",\n",
    "    r\"(it\\s*worked\\s*for\\s*me\\s*but\\s*not\\s*for\\s*everyone)\": \"Hesitation/Side Effects\",\n",
    "    r\"(still\\s*believe\\s*vaccine\\s*works)\": \"Positive Term\",\n",
    "    r\"(hesitant\\s*about\\s*vaccine\\s*but\\s*getting\\s*it)\": \"Hesitation/Side Effects\",\n",
    "    r\"(vaccine\\s*should\\s*be\\s*voluntary)\": \"Mandate & Choice\",\n",
    "    r\"(vaccine\\s*does\\s*not\\s*prevent\\s*certain\\s*strains)\": \"Negative Term\",\n",
    "    r\"(there\\s*are\\s*issues\\s*with\\s*vaccine)\": \"Negative tERM\",\n",
    "    r\"(vaccine\\s*is\\s*a\\s*choice\\s*for\\s*individuals)\": \"Choice & Mandates\",\n",
    "    # Mixed\n",
    "    r\"(vaccine\\s*has\\s*side\\s*effects\\s*(but\\s*is\\s*still\\s*necessary|and\\s*is\\s*worth\\s*the\\s*risks|and\\s*is*minor\\s*compared\\s*to\\s*disease))\": \"Mixed\",\n",
    "    # Specific Groups\n",
    "    r\"(vaccine\\s*is\\s*important\\s*for\\s*(health\\s*care\\s*workers|elderly|those\\s*with\\s*pre-existing\\s*conditions|high\\s*risk\\s*populations))\": \"Specific Groups\",\n",
    "    # Covid & Vaccine Benefits\n",
    "    r\"(covid\\s*is\\s*real\\s*and\\s*vaccine\\s*is\\s*important|covid\\s*should\\s*not\\s*be\\s*taken\\s*lightly|covid\\s*can\\s*kill\\s*and\\s*vaccine\\s*helps)\": \"Covid & Vaccine Benefits\",\n",
    "    r\"(covid\\s*is\\s*serious|covid\\s*can\\s*be\\sdangerous|covid\\s*has\\s*changed\\s*the\\s*world|covid\\s*is\\s*a\\s*threat)\": \"Covid & Vaccine Benefits\",\n",
    "    # Concerns but Open\n",
    "    r\"(concerned\\s*about\\s*vaccine\\s*but\\s*still\\s*getting\\s*it)\": \"Open to Vaccine\",\n",
    "    r\"(vaccine\\s*has\\s*risks\\s*but\\s*is\\s*necessary)\": \"Open to Vaccine\",\n",
    "    r\"(unsure\\s*about\\s*vaccine\\s*but\\s*still\\s*considering\\s*it)\": \"Open to Vaccine\",\n",
    "    # Positive\n",
    "    r\"(?i)(HOORAY\\s*FOR\\s*BEING|feel\\s*good|feel\\s*great|thanks\\s*for\\s*the\\s*compliment|good\\s*day|thank\\s*you|love|works\\s*for\\s*me|exited|built\\s*different|feel\\s*like\\s*doing|positive\\s*effect)\": \"Positive Term\",\n",
    "    # Negative\n",
    "    r\"(?i)(won't\\s*work|isn't\\s*needed|don't\\s*think|can't\\s*believe|never\\s*going\\s*back|no\\s*way\\s*to\\s*prove|does\\s*not\\s*work|not\\s*safe|bad\\s*side\\s*effects|problem\\s*with\\s*vaccine|not\\s*effective|failure\\s*of)\": \"Negative Term\",\n",
    "    # Negation\n",
    "    r\"(?i)(not\\s*work|not\\s*happy|not\\s*good|can't\\s*be|never\\s*needed|don't\\s*agree|can't\\s*accept|didn't\\s*feel|won't\\s*agree|don't\\s*like|not\\s*comfortable)\": \"Negation\",\n",
    "    r\"(?i)(no\\s*problem|no\\s*side\\s*effects|no\\s*issues|won't\\s*affect|not\\s*concerned|didn't\\s*mind|don't\\s*care|not\\s*worried)\": \"Negation\",\n",
    "    # Negation flipping\n",
    "    r\"(?i)\\b(not|never|n't|can't|won't)\\b.*\\b(positive|great|safe|effective|good|happy)\\b\": \"Negation Flipping\",\n",
    "    r\"(?i)\\b(not|never|n't|can't|won't)\\b.*\\b(works|help|successful)\\b\": \"Negation Flipping\",\n",
    "    # Negation flipping\n",
    "    r\"(?i)\\b(not|never|n't|can't|won't)\\b.*\\b(bad|problem|failure|wrong|negative)\\b\": \"Negation Flipping\",\n",
    "    r\"(?i)\\b(not|never|n't|can't|won't)\\b.*\\b(worst|issue|failed|sucks|hate)\\b\": \"Negation Flipping\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "alert_keywords = [\n",
    "    \"critical\",\n",
    "    \"severe\",\n",
    "    \"urgent\",\n",
    "    \"resuscitation\",\n",
    "    \"code blue\",\n",
    "    \"intensive care\",\n",
    "    \"collapsed\",\n",
    "    \"cardiac arrest\",\n",
    "    \"stroke\",\n",
    "    \"trauma\",\n",
    "    \"collapse\",\n",
    "    \"anaphylaxis\",\n",
    "    \"shock\",\n",
    "    \"crisis\",\n",
    "    \"life-threatening\",\n",
    "    \"urgency\",\n",
    "    \"respiratory failure\",\n",
    "    \"unresponsive\",\n",
    "    \"sepsis\",\n",
    "    \"hypoxia\",\n",
    "    \"hemorrhage\",\n",
    "    \"cardiopulmonary arrest\",\n",
    "    \"overdose\",\n",
    "    \"coma\",\n",
    "    \"deteriorating condition\",\n",
    "    \"obstruction\",\n",
    "    \"severe pain\",\n",
    "    \"acute\",\n",
    "    \"malfunction\",\n",
    "    \"choking\",\n",
    "    \"viral outbreak\",\n",
    "    \"paralysis\",\n",
    "    \"collapsed lung\",\n",
    "    \"arrest\",\n",
    "    \"suffocation\",\n",
    "    \"bleeding\",\n",
    "    \"emergency\",\n",
    "    \"icu\",\n",
    "    \"death\",\n",
    "    \"threat\",\n",
    "]\n",
    "\n",
    "\n",
    "severity_mapping = {\n",
    "    'headache': 1,\n",
    "    'sore arm': 1,\n",
    "    'dizziness': 1,\n",
    "    'nausea': 1,\n",
    "    'fatigue': 1,\n",
    "    'brain fog': 1,\n",
    "    'muscle aches': 1,\n",
    "    'chills': 1,\n",
    "    'feeling unwell': 1,\n",
    "    'fever': 2,\n",
    "    'muscle soreness': 2,\n",
    "    'joint pain': 2,\n",
    "    'shortness of breath': 2,\n",
    "    'extreme fatigue': 3,\n",
    "    'myocarditis': 3,\n",
    "    'stroke': 4,\n",
    "    'death': 5,\n",
    "    'critical': 0,\n",
    "    'severe': 0,\n",
    "    'urgent': 0,\n",
    "    'resuscitation': 0,\n",
    "    'code blue': 0,\n",
    "    'intensive care': 0,\n",
    "    'collapsed': 0,\n",
    "    'cardiac arrest': 0,\n",
    "    'trauma': 0,\n",
    "    'collapse': 0,\n",
    "    'anaphylaxis': 0,\n",
    "    'shock': 0,\n",
    "    'crisis': 0,\n",
    "    'life-threatening': 0,\n",
    "    'urgency': 0,\n",
    "    'respiratory failure': 0,\n",
    "    'unresponsive': 0,\n",
    "    'sepsis': 0,\n",
    "    'hypoxia': 0,\n",
    "    'hemorrhage': 0,\n",
    "    'cardiopulmonary arrest': 0,\n",
    "    'overdose': 0,\n",
    "    'coma': 0,\n",
    "    'deteriorating condition': 0,\n",
    "    'obstruction': 0,\n",
    "    'severe pain': 0,\n",
    "    'acute': 0,\n",
    "    'malfunction': 0,\n",
    "    'choking': 0,\n",
    "    'viral outbreak': 0,\n",
    "    'paralysis': 0,\n",
    "    'collapsed lung': 0,\n",
    "    'arrest': 0,\n",
    "    'suffocation': 0,\n",
    "    'bleeding': 0,\n",
    "    'emergency': 0,\n",
    "    'icu': 0,\n",
    "    'death': 5,\n",
    "    'threat': 0\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad3dc77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5df0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Function to clean the tweet text, removing unwanted characters, URLs, etc.\n",
    "def clean_text(text):\n",
    "    # Ensure the input is a string (if not, convert it to an empty string or use a default)\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"  # Return an empty string instead of NaN for non-string values\n",
    "\n",
    "    # Remove URLs (http, www, https)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "\n",
    "    # Remove mentions (e.g., @username)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "    # Remove special characters, numbers, and keep only alphabets and spaces\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "\n",
    "    # Remove extra spaces between words\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Return cleaned text\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "df['cleaned_text'] = df['Text0'].apply(lambda x: clean_text(x))\n",
    "\n",
    "\n",
    "# Function to normalize slang\n",
    "def normalize_slang(text):\n",
    "    if not isinstance(text, str):  # Check if the text is a string\n",
    "        return []  # Return an empty list if it's not a string\n",
    "\n",
    "    for slang, symptoms in slang_lexicon.items():\n",
    "        if slang in text:\n",
    "            return symptoms\n",
    "    return []\n",
    "\n",
    "# Apply slang normalization\n",
    "df['normalized_symptoms'] = df['cleaned_text'].apply(normalize_slang)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert the 'Date' column to datetime (specifying the format as MM/DD/YYYY)\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%Y')\n",
    "\n",
    "# Extract year, month, day, hour, and minute from the 'Date' column\n",
    "df['year'] = df['Date'].dt.year\n",
    "df['month'] = df['Date'].dt.month\n",
    "df['day'] = df['Date'].dt.day\n",
    "df['hour'] = df['Date'].dt.hour  # 'hour' instead of 'Hours'\n",
    "\n",
    "# Combine Year, Month, Day, Hour, and Minute columns into a datetime object\n",
    "df['datetime'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']])\n",
    "\n",
    "# Extract hour and day of the week from the 'datetime' column\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['day_of_week'] = df['datetime'].dt.day_name()\n",
    "\n",
    "df.head(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3500fa04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154984a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'df' is your DataFrame and the 'Date' column is in 'YYYY-MM-DD' format\n",
    "\n",
    "# Convert the 'Date' column to datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Extract the day of the week from the 'Date' column (0 = Monday, 6 = Sunday)\n",
    "df['DayOfWeek'] = df['Date'].dt.day_name()\n",
    "\n",
    "# Count the occurrences of each day of the week\n",
    "day_of_week_counts = df['DayOfWeek'].value_counts()\n",
    "\n",
    "# Define the correct order for weekdays\n",
    "weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "# Reindex to ensure all days of the week are included, even if some days are missing\n",
    "day_of_week_counts = day_of_week_counts.reindex(weekday_order, fill_value=0)\n",
    "\n",
    "\n",
    "# **Step: Plot the results as a bar plot**\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size for better readability\n",
    "day_of_week_counts.plot(kind='bar', color='skyblue', edgecolor='black')  # Create bar plot\n",
    "plt.title('Occurrences of Each Day of the Week')  # Add a title\n",
    "plt.xlabel('Day of the Week')  # Label for the x-axis\n",
    "plt.ylabel('Count')  # Label for the y-axis\n",
    "plt.xticks(rotation=45)  # Rotate the x-axis labels for better readability\n",
    "plt.show()  # Display the plot\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# Example DataFrame (df) with a 'Text0' column containing tweets\n",
    "# df = pd.read_csv('your_file.csv')  # Un-comment to read from a file\n",
    "\n",
    "# Words and phrases to search for in the tweets\n",
    "keywords = [\n",
    "    'covid',\n",
    "    'feeling',\n",
    "    'vaccine',\n",
    "    'covid vaccine',\n",
    "    'side effect',\n",
    "    'adverse effect',\n",
    "    'adverse event',\n",
    "    'covid vaccine side effect',\n",
    "    'covid vaccine adverse effect',\n",
    "    'covid vaccine adverse event',\n",
    "]\n",
    "\n",
    "# Add a check for words starting with #\n",
    "keywords.append(r'\\#\\w+')\n",
    "\n",
    "# Create a boolean DataFrame with True/False values for each keyword\n",
    "keyword_counts = {keyword: df['Text0'].str.contains(keyword, case=False, na=False) for keyword in keywords}\n",
    "\n",
    "# Add a column for tweets that do not contain any of the keywords\n",
    "keyword_counts['none_of_these'] = ~df['Text0'].str.contains('|'.join(keywords), case=False, na=False)\n",
    "\n",
    "# Convert the boolean values into integers (True = 1, False = 0)\n",
    "keyword_counts = pd.DataFrame(keyword_counts).astype(int)\n",
    "\n",
    "# Calculate the count of tweets containing each keyword or group of keywords\n",
    "keyword_sums = keyword_counts.sum()\n",
    "\n",
    "# Filter out any keywords with a count of zero\n",
    "keyword_sums = keyword_sums[keyword_sums > 0]\n",
    "\n",
    "# Sort the keyword sums in descending order\n",
    "keyword_sums = keyword_sums.sort_values(ascending=False)\n",
    "\n",
    "# Set the display option to avoid scientific notation in y-axis\n",
    "plt.figure(figsize=(8, 5))  # Adjust the figure size to make it compact\n",
    "ax = keyword_sums.plot(kind='bar', stacked=True)\n",
    "\n",
    "# Customize the y-axis to avoid scientific notation\n",
    "ax.yaxis.set_major_locator(MaxNLocator(integer=True, prune='lower'))\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Count of Tweets Containing Specific Keywords or Phrases')\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Count of Tweets')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels to fit better\n",
    "plt.tight_layout()  # Adjust layout to remove unnecessary whitespace\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Now count the total number of words starting with #\n",
    "# Use regex to find all words starting with #\n",
    "hashtag_words = df['Text0'].str.findall(r'\\#\\w+')\n",
    "\n",
    "# Flatten the list of lists and count the total number of words\n",
    "total_hashtag_words = sum(len(words) for words in hashtag_words)\n",
    "\n",
    "# Print the total count of words starting with #\n",
    "print(f\"Total count of words starting with # in the entire 'Text0' column: {total_hashtag_words}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# Assuming you have the DataFrame 'df' with necessary columns\n",
    "\n",
    "# Ensure the 'Date' column is in datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%Y', errors='coerce')\n",
    "df['month'] = df['Date'].dt.month  # Extract month if not already present\n",
    "\n",
    "# Sort data by Date to apply rolling average correctly\n",
    "df = df.sort_values(by='Date')\n",
    "\n",
    "# Set larger font sizes globally for the plots\n",
    "sns.set_context(\"notebook\", font_scale=1.5)  # Increase the font size of labels and titles\n",
    "\n",
    "# Ensure all months are represented in the data (even those without records)\n",
    "all_months = pd.Series(range(1, 13))\n",
    "monthly_data = df.groupby('month').size().reset_index(name='count')\n",
    "monthly_data = all_months.to_frame(name='month').merge(monthly_data, on='month', how='left')\n",
    "\n",
    "# Plot 1: Count of data (number of rows) per month\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=monthly_data['month'], y=monthly_data['count'], palette='Blues')\n",
    "plt.title(\"Count of Data per Month\", fontsize=18)\n",
    "plt.xlabel(\"Month\", fontsize=19)\n",
    "plt.ylabel(\"Count of Data\", fontsize=19)\n",
    "plt.xticks(rotation=45, fontsize=19)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# **Step 3: Sentiment Analysis**\n",
    "# In this step, we perform sentiment analysis on the cleaned tweet text to classify the sentiment\n",
    "# as either \"positive\", \"negative\", or \"neutral\". We use the TextBlob library to analyze the text\n",
    "# and determine the sentiment polarity, which is a value ranging from -1 (very negative) to 1 (very positive).\n",
    "# Sentiment is assigned based on the polarity:\n",
    "#   - Positive sentiment is assigned if polarity is greater than 0\n",
    "#   - Negative sentiment is assigned if polarity is less than 0\n",
    "#   - Neutral sentiment is assigned if polarity equals 0 (neutral)\n",
    "#\n",
    "# The `get_sentiment()` function ensures that the input text is a string, performs sentiment analysis using TextBlob,\n",
    "# and categorizes the sentiment based on the polarity score. The function is applied to each tweet in the 'cleaned_text'\n",
    "# column, and the resulting sentiment labels (positive, negative, neutral) are stored in a new column called 'sentiment'.\n",
    "\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Function to get sentiment (positive, neutral, negative) from text\n",
    "def get_sentiment(text):\n",
    "    # Ensure the input is a string\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"  # Replace non-string with empty string or some default text\n",
    "\n",
    "    # Perform sentiment analysis\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity  # Sentiment polarity: ranges from -1 (negative) to 1 (positive)\n",
    "\n",
    "    # Assign sentiment categories based on polarity\n",
    "    if polarity > 0:\n",
    "        return 'positive'\n",
    "    elif polarity < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Apply sentiment analysis to each tweet in 'Text0' column\n",
    "df['sentiment'] = df['cleaned_text'].apply(get_sentiment)\n",
    "\n",
    "# Check sentiment distribution\n",
    "print(df['sentiment'].value_counts())  # To see if there are actually any neutral, positive, negative sentiments\n",
    "\n",
    "# Check sample text and sentiment results to ensure correctness\n",
    "print(df[['cleaned_text', 'sentiment']].head())  # Preview the first few rows\n",
    "\n",
    "# Plot sentiment distribution as a histogram\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "\n",
    "# Create a bar plot to show the distribution of sentiment categories\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values.astype(int), palette='Set2')\n",
    "\n",
    "# Customize plot labels and title\n",
    "plt.title('Sentiment Distribution of Tweets', fontsize=16)\n",
    "plt.xlabel('Sentiment', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Assuming df is your DataFrame and 'known_symptoms', 'uncommon_vaccine_side_effects_lexicon', and 'alert_keywords' are predefined\n",
    "\n",
    "# Function to extract symptoms based on a lexicon (common or uncommon side effects)\n",
    "def extract_symptoms(text, lexicon=known_symptoms):\n",
    "    if not isinstance(text, str):  # Ensure text is a string\n",
    "        return []\n",
    "\n",
    "    extracted = []\n",
    "    # Check for known symptoms in the lexicon\n",
    "    for symptom in lexicon:\n",
    "        if re.search(r'\\b' + re.escape(symptom) + r'\\b', text.lower()):\n",
    "            extracted.append(symptom)\n",
    "\n",
    "    return extracted\n",
    "\n",
    "# Function to extract alert keywords based on the alert lexicon\n",
    "def extract_alert_keywords(text, lexicon=alert_keywords):\n",
    "    if not isinstance(text, str):  # Ensure text is a string\n",
    "        return []\n",
    "\n",
    "    extracted = []\n",
    "    # Check for alert keywords in the lexicon\n",
    "    for alert in lexicon:\n",
    "        if re.search(r'\\b' + re.escape(alert) + r'\\b', text.lower()):\n",
    "            extracted.append(alert)\n",
    "\n",
    "    return extracted\n",
    "\n",
    "# Apply symptom extraction to each tweet (assuming 'Text0' is the tweet text)\n",
    "df['extracted_symptoms'] = df['Text0'].apply(lambda x: extract_symptoms(x))\n",
    "\n",
    "# Create a flag indicating whether any known symptoms were mentioned in the tweet\n",
    "df['known_symptoms_flag'] = df['extracted_symptoms'].apply(lambda x: 1 if x else 0)\n",
    "\n",
    "# Create a flag indicating whether any uncommon symptoms were mentioned in the tweet\n",
    "def extract_uncommon_symptoms(text, lexicon=uncommon_vaccine_side_effects_lexicon):\n",
    "    if not isinstance(text, str):  # Ensure text is a string\n",
    "        return []\n",
    "\n",
    "    extracted = []\n",
    "    # Check for uncommon symptoms in the lexicon\n",
    "    for symptom in lexicon:\n",
    "        if re.search(r'\\b' + re.escape(symptom) + r'\\b', text.lower()):\n",
    "            extracted.append(symptom)\n",
    "\n",
    "    return extracted\n",
    "\n",
    "# Apply uncommon symptom extraction to each tweet (assuming 'Text0' is the tweet text)\n",
    "df['extracted_uncommon_symptoms'] = df['Text0'].apply(lambda x: extract_uncommon_symptoms(x))\n",
    "\n",
    "# Create a flag indicating whether any uncommon symptoms were mentioned in the tweet\n",
    "df['uncommon_symptoms_flag'] = df['extracted_uncommon_symptoms'].apply(lambda x: 1 if x else 0)\n",
    "\n",
    "# Apply alert keyword extraction to each tweet\n",
    "df['extracted_alert_keywords'] = df['Text0'].apply(lambda x: extract_alert_keywords(x))\n",
    "\n",
    "# Create a flag indicating whether any alert keywords were mentioned in the tweet\n",
    "df['alert_keywords_flag'] = df['extracted_alert_keywords'].apply(lambda x: 1 if x else 0)\n",
    "\n",
    "# Save the results to a CSV file without overwriting the original data\n",
    "#df[['extracted_symptoms', 'extracted_uncommon_symptoms', 'extracted_alert_keywords', 'known_symptoms_flag', 'uncommon_symptoms_flag', 'alert_keywords_flag']].to_csv(\n",
    "#    'sideeffects_100.csv', index=False)\n",
    "\n",
    "# Visualization 1: Count of each known symptom detected in the dataset\n",
    "all_known_symptoms = [symptom for sublist in df['extracted_symptoms'] for symptom in sublist]\n",
    "\n",
    "# Plot bar chart of the top 10 symptom frequencies\n",
    "plt.figure(figsize=(10, 6))\n",
    "symptom_counts = pd.Series(all_known_symptoms).value_counts()\n",
    "# Get the top 10 most frequent symptoms\n",
    "top_10_symptoms = symptom_counts.head(10)\n",
    "# Plot the bar chart for the top 10 symptoms\n",
    "sns.barplot(x=top_10_symptoms.index, y=top_10_symptoms.values, palette=\"viridis\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Top 10 Known Vaccine Side Effects in Text Data\")\n",
    "plt.xlabel(\"Symptom\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualization 2: Bar chart for uncommon vaccine side effects (using 'uncommon_vaccine_side_effects_lexicon')\n",
    "all_uncommon_symptoms = [symptom for sublist in df['extracted_uncommon_symptoms'] for symptom in sublist]\n",
    "\n",
    "# Plot bar chart of the top 10 uncommon symptom frequencies\n",
    "plt.figure(figsize=(10, 6))\n",
    "uncommon_symptom_counts = pd.Series(all_uncommon_symptoms).value_counts()\n",
    "# Get the top 10 most frequent uncommon symptoms\n",
    "top_10_uncommon_symptoms = uncommon_symptom_counts.head(10)\n",
    "# Plot the bar chart for the top 10 uncommon symptoms\n",
    "sns.barplot(x=top_10_uncommon_symptoms.index, y=top_10_uncommon_symptoms.values, palette=\"plasma\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Top 10 Uncommon Vaccine Side Effects in Text Data\")\n",
    "plt.xlabel(\"Uncommon Symptom\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualization 3: Pie chart for distribution of texts with Common Symptoms, Uncommon Symptoms, and Alerts\n",
    "common_symptoms_count = (df['known_symptoms_flag'] == 1).sum()\n",
    "uncommon_symptoms_count = (df['uncommon_symptoms_flag'] == 1).sum()\n",
    "alert_keywords_count = (df['alert_keywords_flag'] == 1).sum()\n",
    "\n",
    "# Create labels for common, uncommon, and alert keywords\n",
    "labels = [\"Common Symptoms\", \"Uncommon Symptoms\", \"Alert Keywords\"]\n",
    "\n",
    "# Plot the pie chart for tweets with common, uncommon symptoms, and alert keywords\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.pie([common_symptoms_count, uncommon_symptoms_count, alert_keywords_count], labels=labels,\n",
    "        autopct='%1.1f%%', startangle=90, colors=[\"#ff9999\", \"#66b3ff\", \"#ffcc00\"])\n",
    "plt.title(\"Proportion of Tweets with Common Symptoms vs. Uncommon Symptoms vs. Alerts\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Assuming your DataFrame df is already loaded with the necessary data and columns\n",
    "\n",
    "# Function to extract timestamp mentions and track onset (from the Text0 column)\n",
    "def extract_onset_time(text):\n",
    "    if not isinstance(text, str):  # Ensure text is a string\n",
    "        return []  # Return empty list for non-string values (including NaN/None)\n",
    "\n",
    "    # Define replacement for non-numeric time values (e.g., \"a\", \"few\", \"couple\")\n",
    "    replacements = {\n",
    "        \"a\": 1,           # \"a\" becomes 1 hour\n",
    "        \"few\": 2,         # \"few\" becomes 2 hours\n",
    "        \"couple\": 2,      # \"couple\" becomes 2 hours\n",
    "        \"several\": 3,     # \"several\" becomes 3 hours\n",
    "        \"many\": 5,        # \"many\" becomes 5 hours\n",
    "        \"dozen\": 12,      # \"dozen\" becomes 12 hours\n",
    "        \"half\": 0.5,      # \"half\" becomes 0.5 hours\n",
    "        \"long\": 8,        # \"long\" becomes 8 hours\n",
    "        \"short\": 1,       # \"short\" becomes 1 hour\n",
    "        \"some\": 3,        # \"some\" becomes 3 hours\n",
    "        \"next\": 24,       # \"next\" becomes 24 hours (1 day)\n",
    "        \"last\": 24,       # \"last\" becomes 24 hours (1 day)\n",
    "        \"immediate\": 0,   # \"immediate\" becomes 0 hours (instantaneous)\n",
    "        \"soon\": 1,        # \"soon\" becomes 1 hour\n",
    "        \"this\": 1,        # \"this\" becomes 1 hour\n",
    "        \"after\": 1,       # \"after\" becomes 1 hour\n",
    "        \"before\": 1,      # \"before\" becomes 1 hour\n",
    "        \"morning\": 6,     # \"morning\" becomes 6 hours\n",
    "        \"afternoon\": 6,   # \"afternoon\" becomes 6 hours\n",
    "        \"evening\": 6,     # \"evening\" becomes 6 hours\n",
    "        \"night\": 8,       # \"night\" becomes 8 hours\n",
    "        \"week\": 168,      # \"week\" becomes 168 hours (7 days)\n",
    "        \"month\": 730,     # \"month\" becomes 730 hours (30 days)\n",
    "    }\n",
    "\n",
    "\n",
    "    time_patterns = [\n",
    "    # Patterns for Sentiment Analysis related to Vaccines\n",
    "    r\"(not\\s*get\\s*vaccine)\",\n",
    "    r\"(not\\s*believe\\s*in\\s*vaccine)\",\n",
    "    r\"(vaccine\\s*(has\\s*no\\s*side\\s*effects|was\\s*effective))\",\n",
    "    r\"(vaccine\\s*(should\\s*be\\s*mandatory|should\\s*be\\s*choice))\",\n",
    "    r\"(i\\s*(am\\s*scared|am\\s*hesitant)\\s*about\\s*vaccine)\",\n",
    "    r\"(vaccine\\s*does\\s*not\\s*prevent\\s*covid)\",\n",
    "    r\"(vaccine\\s*gave\\s*me\\s*side\\s*effects)\",\n",
    "    r\"(vaccine\\s*is\\s*a\\s*choice\\s*not\\s*a\\s*mandate)\",\n",
    "    r\"(vaccine\\s*(is\\s*helping|has\\s*been\\s*effective))\",\n",
    "    r\"(vaccine\\s*has\\s*caused\\s*issues|vaccine\\s*causes\\s*harm)\",\n",
    "    r\"(vaccine\\s*is\\s*safe|vaccine\\s*has\\s*worked\\s*for\\s*me)\",\n",
    "    r\"(vaccine\\s*(does\\s*not\\s*work|does\\s*not\\s*prevent\\s*covid))\",\n",
    "    r\"(not\\s*going\\s*to\\s*take\\s*vaccine)\",\n",
    "    r\"(vaccine\\s*has\\s*side\\s*effects\\s*(but\\s*still\\s*taking\\s*it|had\\s*no\\s*problem))\",\n",
    "    r\"(do\\s*not\\s*want\\s*the\\s*vaccine)\",\n",
    "    r\"(vaccinated\\s*and\\s*still\\s*get\\s*covid)\",\n",
    "    r\"(side\\s*effects\\s*from\\s*vaccine)\",\n",
    "    r\"(i\\s*feel\\s*confident\\s*after\\s*vaccine)\",\n",
    "    r\"(it\\s*worked\\s*for\\s*me\\s*but\\s*not\\s*for\\s*everyone)\",\n",
    "    r\"(still\\s*believe\\s*vaccine\\s*works)\",\n",
    "    r\"(hesitant\\s*about\\s*vaccine\\s*but\\s*getting\\s*it)\",\n",
    "    r\"(vaccine\\s*should\\s*be\\s*voluntary)\",\n",
    "    r\"(vaccine\\s*does\\s*not\\s*prevent\\s*certain\\s*strains)\",\n",
    "    r\"(there\\s*are\\s*issues\\s*with\\s*vaccine)\",\n",
    "    r\"(vaccine\\s*is\\s*a\\s*choice\\s*for\\s*individuals)\",\n",
    "\n",
    "    # Mandate and Forced Vaccine (Mandate is forced and unwanted)\n",
    "    r\"((vaccine\\s*mandates\\s*(are|should\\s*be)\\s*(necessary|unnecessary|unconstitutional|forced|wrong)|vaccine\\s*passport\\s*(is|should\\s*be)\\s*(illegal|unethical|infringing|wrong)))\",\n",
    "\n",
    "    # Negation of Vaccine Mandates\n",
    "    r\"(\\b(not\\s*enough\\s*vaccine\\s*availability|vaccine\\s*mandates\\s*should\\s*not\\s*be\\s*forced|vaccine\\s*mandates\\s*should\\s*be\\s*optional))\",\n",
    "\n",
    "    # Personal choice & free will sentiment\n",
    "    r\"((vaccine\\s*is\\s*personal\\s*choice|vaccine\\s*(should\\s*not\\s*be)\\s*forced|vaccine\\s*is\\s*optional))\",\n",
    "\n",
    "    # Positive (Belief in Science/Trust in Vaccine)\n",
    "    r\"(vaccine\\s*is\\s*backed\\s*by\\s*science|vaccine\\s*reduces\\s*infection\\s*risk|vaccine\\s*(protects|saves)\\s*lives|vaccine\\s*is\\s*scientifically\\s*proven)\",\n",
    "\n",
    "    # Negative (Skepticism about Science/Belief)\n",
    "    r\"(vaccine\\s*is\\s*unproven|vaccine\\s*is\\s*unsafe|vaccine\\s*is\\s*experimental|vaccine\\s*is\\s*just\\s*political)\",\n",
    "\n",
    "    # Sentiment where \"not\" negates positive claims (e.g., negating vaccine benefits)\n",
    "    r\"(\\bnot\\s*effective\\s*against\\s*covid|vaccine\\s*does\\s*not\\s*prevent\\s*covid|vaccine\\s*does\\s*not\\s*reduce\\s*spread)\",\n",
    "\n",
    "    # Mixed Sentiment (Vaccination + Side effects)\n",
    "    r\"(vaccine\\s*has\\s*side\\s*effects\\s*(but\\s*is\\s*still\\s*necessary|and\\s*is\\s*worth\\s*the\\s*risks|and\\s*is\\s*minor\\s*compared\\s*to\\s*disease))\",\n",
    "\n",
    "    # Sentiment in Relation to Specific Groups (vaccine is important for some people, not for others)\n",
    "    r\"(vaccine\\s*is\\s*important\\s*for\\s*(health\\s*care\\s*workers|elderly|those\\s*with\\s*pre-existing\\s*conditions|high\\s*risk\\s*populations))\",\n",
    "\n",
    "    # Sentiment on Covid & Vaccine Benefits\n",
    "    r\"(covid\\s*is\\s*real\\s*and\\s*vaccine\\s*is\\s*important|covid\\s*should\\s*not\\s*be\\s*taken\\s*lightly|covid\\s*can\\s*kill\\s*and\\s*vaccine\\s*helps)\",\n",
    "    r\"(covid\\s*is\\s*serious|covid\\s*can\\s*be\\sdangerous|covid\\s*has\\s*changed\\s*the\\s*world|covid\\s*is\\s*a\\s*threat)\",\n",
    "\n",
    "    # Patterns related to concerns and skepticism but open to vaccination\n",
    "    r\"(concerned\\s*about\\s*vaccine\\s*but\\s*still\\s*getting\\s*it)\",\n",
    "    r\"(vaccine\\s*has\\s*risks\\s*but\\s*is\\s*necessary)\",\n",
    "    r\"(unsure\\s*about\\s*vaccine\\s*but\\s*still\\s*considering\\s*it)\",\n",
    "\n",
    "    # Positive Sentiment with no negation\n",
    "    r\"(?i)(HOORAY\\s*FOR\\s*BEING|feel\\s*good|feel\\s*great|thanks\\s*for\\s*the\\s*compliment|good\\s*day|thank\\s*you|love|works\\s*for\\s*me|exited|built\\s*different|feel\\s*like\\s*doing|positive\\s*effect)\",\n",
    "\n",
    "    # Negative Sentiment Patterns\n",
    "    r\"(?i)(won't\\s*work|isn't\\s*needed|don't\\s*think|can't\\s*believe|never\\s*going\\s*back|no\\s*way\\s*to\\s*prove|does\\s*not\\s*work|not\\s*safe|bad\\s*side\\s*effects|problem\\s*with\\s*vaccine|not\\s*effective|failure\\s*of)\",\n",
    "\n",
    "    # Patterns for negation affecting sentiment\n",
    "    r\"(?i)(not\\s*work|not\\s*happy|not\\s*good|can't\\s*be|never\\s*needed|don't\\s*agree|can't\\s*accept|didn't\\s*feel|won't\\s*agree|don't\\s*like|not\\s*comfortable)\",\n",
    "    r\"(?i)(no\\s*problem|no\\s*side\\s*effects|no\\s*issues|won't\\s*affect|not\\s*concerned|didn't\\s*mind|don't\\s*care|not\\s*worried)\",\n",
    "\n",
    "    # Detect negation flipping positive sentiment\n",
    "    r\"(?i)\\b(not|never|n't|can't|won't)\\b.*\\b(positive|great|safe|effective|good|happy)\\b\",\n",
    "    r\"(?i)\\b(not|never|n't|can't|won't)\\b.*\\b(works|help|successful)\\b\",\n",
    "\n",
    "    # Detect negation flipping negative sentiment\n",
    "    r\"(?i)\\b(not|never|n't|can't|won't)\\b.*\\b(bad|problem|failure|wrong|negative)\\b\",\n",
    "    r\"(?i)\\b(not|never|n't|can't|won't)\\b.*\\b(worst|issue|failed|sucks|hate)\\b\",\n",
    "\n",
    "    # Time patterns (Modified from first lexicon to match the second lexicon style)\n",
    "    r\"(\\d+)\\s*(hours?|days?)\\s*(post-dose|after\\s*vaccination|after\\s*shot|after\\s*injection|post-vaccine|post\\s*jab)\",\n",
    "    r\"(\\d+)\\s*(hour|day)\\s*(after|since|post|following)\",\n",
    "    r\"(\\d+)\\s*(hrs?|days?)\\s*(post-vaccine|after\\s*vaccine|post\\s*shot)\",\n",
    "    r\"(\\d+)\\s*(hours?|days?)\\s*(from\\s*injection|from\\s*vaccination)\",\n",
    "    r\"(\\d+)\\s*(hr|day)\\s*(post\\s*administered|after\\s*administered)\",\n",
    "    r\"(\\d+)\\s*(hrs?|days?)\\s*(following\\s*vaccination|following\\s*shot)\",\n",
    "    r\"(\\d+)\\s*(hour|day)\\s*(after\\s*the\\s*vaccine|after\\s*the\\s*shot)\",\n",
    "\n",
    "    # Additional time-related patterns for casual phrasing and specific intervals\n",
    "    r\"(\\d+)\\s*(hour|day|minute|week)\\s*(after\\s*the\\s*vaccine|post\\s*vaccine|after\\s*the\\s*shot|post\\s*shot)\",\n",
    "    r\"(a|an)\\s*(hour|day|minute|week)\\s*(post\\s*vaccination|following\\s*vaccination)\",\n",
    "    r\"(a|few|couple)\\s*(hour|day)\\s*(after|post|since)\\s*(dose|vaccination|shot)\",\n",
    "    r\"(\\d+)\\s*(hours?|days?|weeks?)\\s*(since|after|from)\\s*(dose|vaccination|shot)\",\n",
    "    r\"(\\d+)\\s*(hrs?|days?|weeks?)\\s*(since\\s*the\\s*injection|since\\s*the\\s*vaccine)\",\n",
    "    r\"within\\s*(\\d+)\\s*(hour|day|week)\\s*(post\\s*vaccination|post\\s*shot)\",\n",
    "    r\"(\\d+)\\s*(minutes?|hrs?|days?|weeks?)\\s*(after\\s*administered)\",\n",
    "    r\"(within|after)\\s*(a\\s*few|a\\s*couple)\\s*(hour|day)\\s*(post\\s*vaccine|post\\s*shot)\"\n",
    "]\n",
    "\n",
    "\n",
    "    onset_times = []\n",
    "    for pattern in time_patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            # Extract time and convert to numeric\n",
    "            value = match.group(1).lower()  # Match group as a lowercase string\n",
    "\n",
    "            # If the value is one of the non-numeric words, replace it\n",
    "            if value in replacements:\n",
    "                onset_times.append(replacements[value])\n",
    "            else:\n",
    "                try:\n",
    "                    onset_times.append(int(value))  # Try converting numeric values\n",
    "                except ValueError:\n",
    "                    continue  # In case of unexpected non-numeric values\n",
    "\n",
    "    return onset_times\n",
    "\n",
    "# Function to analyze symptom duration based on timestamps\n",
    "def calculate_duration(timestamp, onset_times):\n",
    "    symptom_duration = []\n",
    "    for onset in onset_times:\n",
    "        # Ensure onset is numeric (hours or days)\n",
    "        if isinstance(onset, int):\n",
    "            # Calculate the duration based on the onset time (assuming onset is in hours)\n",
    "            duration = timestamp + pd.Timedelta(hours=onset)\n",
    "            symptom_duration.append(duration)\n",
    "    return symptom_duration\n",
    "\n",
    "# Assuming df is already loaded from the CSV file, apply the processing steps:\n",
    "\n",
    "# Step 1: Create the timestamp column from the day, month, and year columns (if needed)\n",
    "df['timestamp'] = pd.to_datetime(df[['year', 'month', 'day']])  # Combine year, month, day\n",
    "# If 'Date' column already contains the full timestamp, you can replace the above line with:\n",
    "# df['timestamp'] = pd.to_datetime(df['Date'], errors='coerce')  # Use the 'Date' column directly\n",
    "\n",
    "# Step 2: Apply the onset extraction function to the 'Text0' column (assumed to contain symptom descriptions)\n",
    "df['onset_time'] = df['Text0'].apply(extract_onset_time)\n",
    "\n",
    "# Step 3: Apply duration calculation to determine symptom duration\n",
    "df['symptom_duration'] = df.apply(lambda row: calculate_duration(row['timestamp'], row['onset_time']), axis=1)\n",
    "\n",
    "# Step 4: (Optional) Save the processed data to a new CSV file\n",
    "# output_csv = '/content/drive/My Drive/processed_vaccine_data.csv'  # Path to save the results\n",
    "# df.to_csv(output_csv, index=False)\n",
    "# print(f\"Processed data saved to {output_csv}\")\n",
    "\n",
    "# Step 5: Plot the results\n",
    "def plot_results(df):\n",
    "    # Plot distribution of onset times\n",
    "    onset_times_flat = [item for sublist in df['onset_time'] for item in sublist]  # Flatten the list of onset times\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(onset_times_flat, kde=True, color='skyblue', bins=10)\n",
    "    plt.title(\"Distribution of Symptom Onset Times (in Hours)\")\n",
    "    plt.xlabel(\"Onset Time (Hours)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print example for onset times plot\n",
    "    example_text_1 = df['Text0'].iloc[0]  # Example of Text0 content for this plot\n",
    "    print(f\"\\nExample Text0 for Onset Time Plot:\\n{example_text_1}\")\n",
    "\n",
    "    # Plot distribution of symptom durations (in hours)\n",
    "    symptom_durations_flat = [item for sublist in df['symptom_duration'] for item in sublist]  # Flatten the list of durations\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(symptom_durations_flat, kde=True, color='salmon', bins=10)\n",
    "    plt.title(\"Distribution of Symptom Durations (in Hours)\")\n",
    "    plt.xlabel(\"Duration (Hours)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print example for symptom durations plot\n",
    "    example_text_2 = df['Text0'].iloc[1]  # Example of Text0 content for this plot\n",
    "    print(f\"\\nExample Text0 for Symptom Duration Plot:\\n{example_text_2}\")\n",
    "\n",
    "    # Plot the count of symptoms reported over time (based on the timestamp)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df['date'] = df['timestamp'].dt.date  # Extract just the date from timestamp\n",
    "    df.groupby('date').size().plot(kind='line', marker='o', color='green')\n",
    "    plt.title(\"Number of Symptom Reports Over Time\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Number of Reports\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Print example for symptoms reported over time plot\n",
    "    example_text_3 = df['Text0'].iloc[2]  # Example of Text0 content for this plot\n",
    "    print(f\"\\nExample Text0 for Symptom Reports Over Time Plot:\\n{example_text_3}\")\n",
    "\n",
    "# Call the plotting function\n",
    "plot_results(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Visualizing tweet activity by hour of the day\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(x='hour', data=df, palette='coolwarm')\n",
    "plt.title('Tweet Activity by Hour (Vaccine Side Effects Mentioned)')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re  # Added for regex matching\n",
    "\n",
    "# Function to extract symptoms based on a lexicon (uncommon side effects)\n",
    "def extract_symptoms(text, lexicon=uncommon_vaccine_side_effects_lexicon):\n",
    "    if not isinstance(text, str):  # Ensure text is a string\n",
    "        return []\n",
    "\n",
    "    extracted = []\n",
    "    # Check for each term in the lexicon\n",
    "    for symptom in lexicon:\n",
    "        if re.search(r'\\b' + re.escape(symptom) + r'\\b', text.lower()):\n",
    "            extracted.append(symptom)\n",
    "\n",
    "    return extracted\n",
    "\n",
    "# Assuming 'df' is your DataFrame and it has a 'Text0' column with tweets\n",
    "df['extracted_symptoms'] = df['Text0'].apply(lambda x: extract_symptoms(x))\n",
    "\n",
    "# Check if there's a 'Date' column and create a 'day_of_week' column from it\n",
    "if 'Date' in df.columns:\n",
    "    # Convert 'Date' column to datetime format (if it's not already)\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "\n",
    "    # Create 'day_of_week' column (0 = Monday, 6 = Sunday)\n",
    "    df['day_of_week'] = df['Date'].dt.dayofweek\n",
    "    # Convert to a more readable format if necessary (optional)\n",
    "    df['day_of_week'] = df['day_of_week'].map({0: 'Monday', 1: 'Tuesday', 2: 'Wednesday',\n",
    "                                               3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'})\n",
    "else:\n",
    "    # If no 'Date' column, simulate one or create one manually (optional)\n",
    "    # For example, here I am creating a 'Date' column starting from a given date\n",
    "    df['Date'] = pd.date_range(start='2023-01-01', periods=len(df), freq='D')  # Change to your logic\n",
    "    df['day_of_week'] = df['Date'].dt.dayofweek\n",
    "    df['day_of_week'] = df['day_of_week'].map({0: 'Monday', 1: 'Tuesday', 2: 'Wednesday',\n",
    "                                               3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'})\n",
    "\n",
    "# Flatten the 'extracted_symptoms' list column\n",
    "df_flat = df.explode('extracted_symptoms')\n",
    "\n",
    "# Group by 'day_of_week' and 'extracted_symptoms', and count occurrences\n",
    "symptom_by_time = df_flat.groupby(['day_of_week', 'extracted_symptoms']).size().unstack().fillna(0)\n",
    "\n",
    "# Define the correct order of the days of the week\n",
    "days_of_week_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "# Reorder the columns and index to make sure the days of the week are in the correct order\n",
    "symptom_by_time = symptom_by_time.reindex(days_of_week_order)\n",
    "\n",
    "# Find the top 10 symptoms based on the total count across all days\n",
    "top_10_symptoms = symptom_by_time.sum(axis=0).nlargest(10).index\n",
    "\n",
    "# Filter the DataFrame to only include the top 10 symptoms\n",
    "symptom_by_time_top_10 = symptom_by_time[top_10_symptoms]\n",
    "\n",
    "# Plot the stacked bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 7))  # Reduced figure size\n",
    "symptom_by_time_top_10.plot(kind='bar', stacked=True, ax=ax)\n",
    "\n",
    "# Title and labels\n",
    "plt.title('Symptom Occurrence Over Days of the Week (Top 10 Symptoms)', fontsize=14)\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "\n",
    "# Rotate x-ticks for better readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Place the legend outside of the plot to avoid cluttering the image\n",
    "plt.legend(title='Symptoms', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Adjust layout to make space for the legend\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate the total occurrences for each day (row sums)\n",
    "total_per_day = symptom_by_time_top_10.sum(axis=1)\n",
    "\n",
    "# Calculate the proportion of each symptom for each day\n",
    "proportions_by_day = symptom_by_time_top_10.div(total_per_day, axis=0)\n",
    "\n",
    "# Display the proportions in tabular form\n",
    "print(\"Proportions of Top 10 Symptoms by Day of the Week:\")\n",
    "print(proportions_by_day)\n",
    "\n",
    "# Optionally, display the proportions in a table in a subplot below the chart\n",
    "fig, ax = plt.subplots(figsize=(12, 7))  # Reduced figure size to match previous chart\n",
    "ax.axis('tight')  # Hide the axis\n",
    "ax.axis('off')  # Hide the axis\n",
    "ax.table(cellText=proportions_by_day.round(2).values,  # Show proportions rounded to 2 decimal places\n",
    "         colLabels=proportions_by_day.columns,\n",
    "         rowLabels=proportions_by_day.index,\n",
    "         loc='center',\n",
    "         cellLoc='center',\n",
    "         colColours=['#f1f1f1']*len(proportions_by_day.columns))  # Optional color for table header\n",
    "\n",
    "# Show the table\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot the heatmap of proportions\n",
    "plt.figure(figsize=(10, 7))  # Slightly smaller figure size for heatmap\n",
    "sns.heatmap(proportions_by_day, annot=True, cmap='YlGnBu', fmt=\".2f\", cbar_kws={'label': 'Proportion'})\n",
    "plt.title('Proportion of Top 10 Symptoms Across Days of the Week', fontsize=14)\n",
    "plt.xlabel('Symptoms', fontsize=12)\n",
    "plt.ylabel('Days of the Week', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Example uncommon side effects lexicon (you should define this list)\n",
    "\n",
    "# Example DataFrame creation (replace with your actual DataFrame)\n",
    "# df = pd.DataFrame({\"Text0\": [\"Feeling feverish after the jab\", \"No side effects after the vaccine\", \"Got the rash after the shot\"]})\n",
    "\n",
    "# Function to extract symptoms based on a lexicon (uncommon side effects)\n",
    "def extract_symptoms(text, lexicon=uncommon_vaccine_side_effects_lexicon):\n",
    "    if not isinstance(text, str):  # Ensure text is a string\n",
    "        return []\n",
    "\n",
    "    extracted = []\n",
    "    # Check for each term in the lexicon\n",
    "    for symptom in lexicon:\n",
    "        if re.search(r'\\b' + re.escape(symptom) + r'\\b', text.lower()):\n",
    "            extracted.append(symptom)\n",
    "\n",
    "    return extracted\n",
    "\n",
    "# Apply symptom extraction\n",
    "df['extracted_symptoms'] = df['Text0'].apply(lambda x: extract_symptoms(x))\n",
    "\n",
    "# Create a flag indicating whether any uncommon side effect was mentioned in the tweet\n",
    "df['uncommon_sideeffects_flag'] = df['extracted_symptoms'].apply(lambda x: 1 if x else 0)\n",
    "\n",
    "# Ensure the column is created before plotting the pie chart\n",
    "side_effect_flag_counts = df['uncommon_sideeffects_flag'].value_counts()\n",
    "\n",
    "# Ensure labels match the number of categories in the counts\n",
    "labels = [\"No Uncommon Side Effects\", \"Has Uncommon Side Effects\"]\n",
    "\n",
    "# Plot the pie chart with the correct number of labels\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.pie(side_effect_flag_counts, labels=labels,\n",
    "        autopct='%1.1f%%', startangle=90, colors=[\"#ff9999\", \"#66b3ff\"])\n",
    "plt.title(\"Distribution of Texts with and Without Uncommon Vaccine Side Effects\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')  # Tokenization resource\n",
    "nltk.download('wordnet')  # Lemmatization resource\n",
    "nltk.download('words')  # English dictionary\n",
    "nltk.download('punkt_tab')  # Additional tokenizer resource (this might be required)\n",
    "\n",
    "# Function to clean text (with error handling for non-string values)\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):  # Check if the value is a string\n",
    "        return \"\"  # Return empty string for non-string values\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters and digits\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    return text\n",
    "\n",
    "# Function to lemmatize the text\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words_in_text = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words_in_text]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Function to check if a word is in the English dictionary\n",
    "def is_english_word(word):\n",
    "    return word in english_words_set\n",
    "\n",
    "# Get the set of English words from NLTK's word corpus\n",
    "english_words_set = set(words.words())\n",
    "\n",
    "# Assuming 'df' is your DataFrame and 'Text0' column exists, you should load your DataFrame before applying operations\n",
    "# Example:\n",
    "# df = pd.read_csv('your_data.csv')  # Make sure you load your data properly\n",
    "\n",
    "# Clean and lemmatize the Text0 column\n",
    "df['cleaned_text'] = df['Text0'].apply(lambda x: clean_text(x))\n",
    "\n",
    "# Check if cleaned_text is valid and apply lemmatization only to valid rows\n",
    "df['lemmatized_text'] = df['cleaned_text'].apply(lambda x: lemmatize_text(x) if isinstance(x, str) else '')\n",
    "\n",
    "# Tokenize the lemmatized text and check if each word is in the dictionary\n",
    "df['word_list'] = df['lemmatized_text'].apply(lambda x: word_tokenize(x) if isinstance(x, str) else [])\n",
    "\n",
    "# Flatten the word list for all tweets into a single list\n",
    "all_words = [word for sublist in df['word_list'] for word in sublist]\n",
    "\n",
    "# Check which words are valid English words\n",
    "valid_words = [word for word in all_words if is_english_word(word)]\n",
    "\n",
    "# Calculate the proportion of valid words\n",
    "valid_word_proportion = len(valid_words) / len(all_words) if len(all_words) > 0 else 0  # Avoid division by zero\n",
    "\n",
    "# Visualize the result using a pie chart\n",
    "labels = ['English Words', 'Non English Words']  # Adjusted label names to avoid \"valid\"\n",
    "\n",
    "sizes = [len(valid_words), len(all_words) - len(valid_words)]\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Increase font size of labels and percentages in the pie chart\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=['lightgreen', 'lightcoral'], startangle=90,\n",
    "        textprops={'fontsize': 15})  # Change textprops for label font size\n",
    "\n",
    "# Title with larger font size\n",
    "plt.title(f\"Proportion of English Words\\n({valid_word_proportion*100:.2f}% English)\", fontsize=18, fontweight='bold')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the proportion of English words\n",
    "print(f\"Proportion of English words: {valid_word_proportion*100:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Assuming df is your DataFrame with 'Text0' column containing the tweets\n",
    "\n",
    "# Function to classify sentiment as Positive, Negative, or Neutral based on polarity\n",
    "def classify_sentiment(text):\n",
    "    # Calculate sentiment polarity using TextBlob\n",
    "    polarity = TextBlob(text).sentiment.polarity\n",
    "\n",
    "    # Classify sentiment based on polarity\n",
    "    if polarity > 0:\n",
    "        return 'Positive'\n",
    "    elif polarity < 0:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Apply the classify_sentiment function to the 'Text0' column to create a new 'sentiment' column\n",
    "df['sentiment'] = df['Text0'].apply(classify_sentiment)\n",
    "\n",
    "# Count the number of tweets in each sentiment category\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette='viridis')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Sentiment', fontsize=12)\n",
    "plt.ylabel('Count of Tweets', fontsize=12)\n",
    "plt.title('Tweet Sentiment Distribution', fontsize=14)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import numpy as np\n",
    "from textblob import TextBlob  # Import TextBlob for sentiment analysis\n",
    "\n",
    "\n",
    "\n",
    "# Function to categorize slang, time patterns, and side effects\n",
    "def categorize_slang_time_sideeffects(text):\n",
    "    matched_slangs = [slang for slang, symptoms in slang_lexicon.items() if any(s in text.lower() for s in symptoms)]\n",
    "    time_pattern_matches = [pattern for pattern in time_patterns if pattern in text.lower()]\n",
    "    side_effect_matches = [symptom for symptom in known_symptoms if symptom in text.lower()]\n",
    "    uncommon_effects_matches = [effect for effect in uncommon_vaccine_side_effects_lexicon if effect in text.lower()]\n",
    "    return matched_slangs, time_pattern_matches, side_effect_matches, uncommon_effects_matches\n",
    "\n",
    "# Apply categorization function to the 'Text0' column\n",
    "df[['slang_matches', 'time_pattern_matches', 'side_effect_matches', 'uncommon_effects_matches']] = df['Text0'].apply(lambda x: pd.Series(categorize_slang_time_sideeffects(x)))\n",
    "\n",
    "# Explode matches to separate rows\n",
    "df_exploded = df.explode('slang_matches').explode('time_pattern_matches').explode('side_effect_matches').explode('uncommon_effects_matches')\n",
    "\n",
    "# Sentiment analysis using TextBlob\n",
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    if polarity > 0:\n",
    "        return 'Positive'\n",
    "    elif polarity < 0:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Apply sentiment analysis to the side effects\n",
    "df_exploded['sentiment'] = df_exploded['side_effect_matches'].apply(lambda x: get_sentiment(x) if isinstance(x, str) else 'Neutral')\n",
    "\n",
    "# 1. Side Effect Frequency with Sentiment - Bar Chart\n",
    "side_effect_sentiment_counts = df_exploded.groupby(['side_effect_matches', 'sentiment']).size().reset_index(name='count')\n",
    "\n",
    "# Adjust plot size and label sizes\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=side_effect_sentiment_counts, x='side_effect_matches', y='count', hue='sentiment', palette='coolwarm')\n",
    "plt.xticks(rotation=90, fontsize=10)  # Reduce the size of x-axis labels\n",
    "plt.xlabel('Side Effect', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Side Effect Frequency with Sentiment', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Side Effect Severity Heatmap (without numbers)\n",
    "df_exploded['severity'] = df_exploded['side_effect_matches'].apply(lambda x: severity_mapping.get(x, np.nan))  # Use severity mapping to assign severity\n",
    "\n",
    "# Group by month and side effect, calculating average severity\n",
    "severity_data = df_exploded.groupby(['month', 'side_effect_matches'])['severity'].mean().unstack().fillna(np.nan)  # Ensure missing values are NaN\n",
    "\n",
    "# Plot heatmap without annotation (numbers)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(severity_data, cmap='YlGnBu', annot=False, cbar_kws={'label': 'Severity'}, linewidths=0.5, mask=severity_data.isna())  # Mask NaN values\n",
    "plt.title('Side Effect Severity Heatmap Across Months', fontsize=16)\n",
    "plt.xlabel('Side Effect', fontsize=12)\n",
    "plt.ylabel('Month', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "# Function to categorize slang terms based on the slang_lexicon\n",
    "def categorize_slang_time_sideeffects(text):\n",
    "    matched_slangs = [slang for slang, symptoms in slang_lexicon.items() if any(s in text.lower() for s in symptoms)]\n",
    "    return matched_slangs\n",
    "\n",
    "# Function to categorize time-based expressions based on time_patterns lexicon\n",
    "def categorize_time_patterns(text):\n",
    "    matched_patterns = [pattern for pattern in time_patterns if re.search(pattern, text, re.IGNORECASE)]\n",
    "    return matched_patterns\n",
    "\n",
    "# Function to apply sentiment analysis using TextBlob\n",
    "def get_sentiment(text):\n",
    "    polarity = TextBlob(text).sentiment.polarity\n",
    "    if polarity > 0:\n",
    "        return 'Positive'\n",
    "    elif polarity < 0:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "\n",
    "# Apply the categorization function to identify slang matches and time-based patterns in the 'Text0' column\n",
    "df['slang_matches'] = df['Text0'].apply(lambda x: categorize_slang_time_sideeffects(x))\n",
    "df['time_matches'] = df['Text0'].apply(lambda x: categorize_time_patterns(x))\n",
    "\n",
    "# Explode slang and time matches into separate rows\n",
    "df_exploded = df.explode('slang_matches').explode('time_matches')\n",
    "\n",
    "# Apply sentiment analysis on the text\n",
    "df_exploded['sentiment'] = df_exploded['Text0'].apply(get_sentiment)\n",
    "\n",
    "# 1. Frequency of Slang Terms with Sentiment and Time Patterns - Bar Chart\n",
    "slang_sentiment_counts = df_exploded.groupby(['slang_matches', 'month', 'sentiment']).size().reset_index(name='count')\n",
    "time_sentiment_counts = df_exploded.groupby(['time_matches', 'month', 'sentiment']).size().reset_index(name='count')\n",
    "\n",
    "# Limit to top 10 slang terms based on count for better visualization\n",
    "top_10_slangs = slang_sentiment_counts.groupby('slang_matches')['count'].sum().nlargest(10).index\n",
    "slang_sentiment_counts = slang_sentiment_counts[slang_sentiment_counts['slang_matches'].isin(top_10_slangs)]\n",
    "\n",
    "top_10_times = time_sentiment_counts.groupby('time_matches')['count'].sum().nlargest(10).index\n",
    "time_sentiment_counts = time_sentiment_counts[time_sentiment_counts['time_matches'].isin(top_10_times)]\n",
    "\n",
    "# Modify x-axis labels to prefix the month number and limit the label length to 30 characters\n",
    "slang_sentiment_counts['x_label'] = slang_sentiment_counts['month'].astype(str) + ' ' + slang_sentiment_counts['slang_matches']\n",
    "slang_sentiment_counts['x_label'] = slang_sentiment_counts['x_label'].apply(lambda x: (x[:30] if len(x) > 30 else x))  # Limit to 30 characters\n",
    "\n",
    "time_sentiment_counts['x_label'] = time_sentiment_counts['month'].astype(str) + ' ' + time_sentiment_counts['time_matches']\n",
    "time_sentiment_counts['x_label'] = time_sentiment_counts['x_label'].apply(lambda x: (x[:30] if len(x) > 30 else x))  # Limit to 30 characters\n",
    "\n",
    "# Plot Frequency of Slang Terms with Sentiment and Month - Stacked Bar Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=slang_sentiment_counts, x='x_label', y='count', hue='sentiment', palette='Set2', ci=None)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Slang Term', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Frequency of Slang Terms with Sentiment and Month', fontsize=16)\n",
    "plt.legend(title=\"Sentiment\", bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10, labels=[(item[:30] + '...') for item in plt.gca().get_legend_handles_labels()[1]])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot Frequency of Time Patterns with Sentiment and Month - Stacked Bar Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=time_sentiment_counts, x='x_label', y='count', hue='sentiment', palette='Set2', ci=None)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Time Pattern', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Frequency of Time Patterns with Sentiment and Month', fontsize=16)\n",
    "plt.legend(title=\"Sentiment\", bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10, labels=[(item[:30] + '...') for item in plt.gca().get_legend_handles_labels()[1]])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Time Series Analysis of Slang Terms Mentions - Line Plot\n",
    "slang_mentions_by_month = df_exploded.groupby(['month', 'slang_matches']).size().reset_index(name='mention_count')\n",
    "\n",
    "# Limit to top 10 slang terms for visualization\n",
    "top_10_slangs_month = slang_mentions_by_month.groupby('slang_matches')['mention_count'].sum().nlargest(10).index\n",
    "slang_mentions_by_month = slang_mentions_by_month[slang_mentions_by_month['slang_matches'].isin(top_10_slangs_month)]\n",
    "\n",
    "# Plot Time Series of Slang Mentions\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.lineplot(data=slang_mentions_by_month, x='month', y='mention_count', hue='slang_matches', marker='o', palette='Set2', lw=2)\n",
    "plt.title('Time Series of Slang Terms Mentions', fontsize=16)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Mention Count', fontsize=12)\n",
    "plt.legend(title='Slang Term', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10, labels=[(item[:30] + '...') for item in plt.gca().get_legend_handles_labels()[1]])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Time Series Analysis of Time Pattern Mentions - Line Plot\n",
    "time_mentions_by_month = df_exploded.groupby(['month', 'time_matches']).size().reset_index(name='mention_count')\n",
    "\n",
    "# Limit to top 10 time patterns for visualization\n",
    "top_10_times_month = time_mentions_by_month.groupby('time_matches')['mention_count'].sum().nlargest(10).index\n",
    "time_mentions_by_month = time_mentions_by_month[time_mentions_by_month['time_matches'].isin(top_10_times_month)]\n",
    "\n",
    "# Plot Time Series of Time Pattern Mentions\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.lineplot(data=time_mentions_by_month, x='month', y='mention_count', hue='time_matches', marker='o', palette='Set2', lw=2)\n",
    "plt.title('Time Series of Time Pattern Mentions', fontsize=16)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Mention Count', fontsize=12)\n",
    "plt.legend(title='Time Pattern', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10, labels=[(item[:30] + '...') for item in plt.gca().get_legend_handles_labels()[1]])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Function to categorize slang lexicon and check for time patterns\n",
    "def categorize_slang_and_time_patterns(text):\n",
    "    # Check for slang matches\n",
    "    matched_slangs = []\n",
    "    for slang, patterns in slang_lexicon.items():\n",
    "        if any(pattern.lower() in text.lower() for pattern in patterns):\n",
    "            matched_slangs.append(slang)\n",
    "\n",
    "    # Check for time pattern matches and map them to human-readable labels\n",
    "    time_pattern_matches = []\n",
    "    for pattern in time_patterns:\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            # Map the pattern to its corresponding label from time_pattern_labels\n",
    "            label = time_pattern_labels.get(pattern, pattern)  # Default to pattern if no label is found\n",
    "            time_pattern_matches.append(label)\n",
    "\n",
    "    return matched_slangs, time_pattern_matches\n",
    "\n",
    "# Function to create the plot based on the dataset\n",
    "def create_plot(dataframe, plot_title):\n",
    "    # Apply the function to categorize slang and detect time patterns in the 'Text0' column\n",
    "    dataframe[['slang_matches', 'time_pattern_matches']] = dataframe['Text0'].apply(lambda x: pd.Series(categorize_slang_and_time_patterns(x)))\n",
    "\n",
    "    # Filter rows with slang matches and avoid exploding\n",
    "    df_slang = dataframe[dataframe['slang_matches'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "    # Exploding the lists into individual rows for slang and time patterns\n",
    "    df_slang_exploded = df_slang.copy()\n",
    "    df_slang_exploded = df_slang_exploded.explode('slang_matches')\n",
    "    df_slang_exploded = df_slang_exploded.explode('time_pattern_matches')\n",
    "\n",
    "    # Add month information\n",
    "    df_slang_exploded['month'] = pd.to_datetime(df_slang_exploded['Date'], errors='coerce').dt.month\n",
    "\n",
    "    # Drop rows with NaT (invalid dates)\n",
    "    df_slang_exploded = df_slang_exploded.dropna(subset=['month'])\n",
    "\n",
    "    # Group by month, slang match, and time pattern to count occurrences\n",
    "    monthly_slang_counts = df_slang_exploded.groupby(['month', 'slang_matches', 'time_pattern_matches']).size().reset_index(name='count')\n",
    "\n",
    "    # Get the top slang expressions per month (top 10 as an example)\n",
    "    top_slangs_per_month = monthly_slang_counts.groupby(['month', 'slang_matches'], group_keys=False).apply(\n",
    "        lambda x: x.nlargest(10, 'count')  # Get top 10 slang expressions per month\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Create a pivot table for the plot\n",
    "    pivot_table = top_slangs_per_month.pivot_table(index=['month', 'slang_matches'],\n",
    "                                                   columns='time_pattern_matches',\n",
    "                                                   values='count',\n",
    "                                                   aggfunc='sum',\n",
    "                                                   fill_value=0)\n",
    "\n",
    "    # Reset the index for easier plotting\n",
    "    pivot_table.reset_index(inplace=True)\n",
    "\n",
    "    # Combine 'month' and 'slang_matches' to create meaningful x-axis labels\n",
    "    pivot_table['month_slang'] = pivot_table['month'].astype(str) + ' ' + pivot_table['slang_matches']\n",
    "\n",
    "    # Plotting the stacked bar plot\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Dynamically filter out columns that do not represent time patterns (e.g., 'month' and 'slang_matches')\n",
    "    time_pattern_columns = [col for col in pivot_table.columns if col not in ['month', 'slang_matches', 'month_slang']]\n",
    "\n",
    "    # Plotting the stacked bar chart using only time pattern columns\n",
    "    pivot_table.set_index('month_slang')[time_pattern_columns].plot(kind='bar', stacked=True, ax=ax1, width=0.8, color=sns.color_palette(\"Set3\", len(time_pattern_columns)))\n",
    "\n",
    "    # Add labels, title, and customizations\n",
    "    ax1.set_xlabel(\"Month & Slang Expression\", fontsize=12)\n",
    "    ax1.set_ylabel(\"Count of Mentions (Time Patterns)\", fontsize=12)\n",
    "    ax1.set_title(plot_title, fontsize=16)\n",
    "\n",
    "    # Truncate legend labels to 40 characters max\n",
    "    truncated_labels = [label if len(label) <= 40 else label[:40] + '...' for label in time_pattern_columns]\n",
    "\n",
    "    # The legend will now show the truncated time pattern labels\n",
    "    ax1.legend(title=\"Time Pattern\", labels=truncated_labels, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    # Adjust x-ticks to show months and slang expressions\n",
    "    ax1.set_xticklabels(pivot_table['month_slang'], rotation=45, ha='right', fontsize=8)\n",
    "\n",
    "    # Tight layout for better spacing\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    return pivot_table  # Return pivot table to save in CSV after the plot\n",
    "\n",
    "# Convert 'Date' column to datetime format (if not already)\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "\n",
    "# Get the latest date in the 'Date' column\n",
    "latest_date = df['Date'].max()\n",
    "\n",
    "# Calculate the date 7 days before the latest date\n",
    "seven_days_ago = latest_date - timedelta(days=30)\n",
    "\n",
    "# Filter the dataframe to include only rows where 'Date' is within the last 7 days from the latest date\n",
    "df_last_7_days = df[df['Date'] >= seven_days_ago]\n",
    "\n",
    "# Apply the function to plot the full dataset\n",
    "pivot_table_full = create_plot(df, \"Top Slang Expressions and Associated Time Patterns for the Entire Dataset\")\n",
    "\n",
    "# Apply the function to plot the last 7 days' data\n",
    "pivot_table_last_7_days = create_plot(df_last_7_days, \"Top Slang Expressions and Associated Time Patterns for the Last 7 Days\")\n",
    "\n",
    "# Now, create the CSV based on the full dataset pivot table\n",
    "# Prepare data for CSV\n",
    "csv_data = []\n",
    "\n",
    "# Iterate over the rows of the pivot_table_full to extract relevant data\n",
    "for idx, row in pivot_table_full.iterrows():\n",
    "    slang_term = row['month_slang']\n",
    "\n",
    "    # Iterate over the time pattern columns (ignoring non-time-pattern columns)\n",
    "    for time_pattern in pivot_table_full.columns[2:]:  # Skip 'month_slang' and 'month' columns\n",
    "        # Convert the count to a numeric value (float) and handle errors gracefully\n",
    "        try:\n",
    "            count = pd.to_numeric(row[time_pattern], errors='coerce')\n",
    "\n",
    "            # Check if the count is greater than 0 (and valid)\n",
    "            if pd.notna(count) and count > 0:  # Only consider non-zero counts\n",
    "                time_pattern_label = time_pattern_labels.get(time_pattern, time_pattern)\n",
    "                csv_data.append([slang_term, time_pattern_label])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row: {row} for time pattern: {time_pattern} - {e}\")\n",
    "\n",
    "# Save the data to a CSV file\n",
    "csv_df = pd.DataFrame(csv_data, columns=['Terms', 'Slang / Time Pattern'])\n",
    "#csv_df.to_csv('sideeffects_training_data_simplematch.csv', index=False)\n",
    "\n",
    "# Display the second plot without saving CSV for the second plot\n",
    "# The second plot already shows within the 'create_plot' function, no further action needed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have the necessary lexicons:\n",
    "# slang_lexicon, common_side_effects, uncommon_vaccine_side_effects_lexicon\n",
    "\n",
    "# Function to clean the text data\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):  # Handle non-string types (e.g., NaN or float)\n",
    "        return \"\"  # Convert non-string to empty string or handle it appropriately\n",
    "\n",
    "    # Clean the text\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabetical characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "\n",
    "    return text\n",
    "\n",
    "# Function to detect side effects based on slang lexicon\n",
    "def detect_side_effects(text, slang_lexicon, uncommon_lexicon):\n",
    "    if not isinstance(text, str):  # Handle non-string inputs (e.g., NaN)\n",
    "        return []  # Return an empty list if it's not a valid string\n",
    "\n",
    "    detected_side_effects = []\n",
    "\n",
    "    # First check the slang lexicon for common side effects\n",
    "    for slang, effects in slang_lexicon.items():\n",
    "        if slang in text.lower():\n",
    "            detected_side_effects.extend(effects)\n",
    "\n",
    "    # Then check the uncommon side effects lexicon\n",
    "    for effect in uncommon_lexicon:\n",
    "        if effect in text.lower():\n",
    "            detected_side_effects.append(effect)\n",
    "\n",
    "    return list(set(detected_side_effects))  # Remove duplicates and return the list\n",
    "\n",
    "# Clean the text and create a new 'cleaned_text' column\n",
    "df['cleaned_text'] = df['Text0'].apply(lambda x: clean_text(x))\n",
    "\n",
    "# Apply the side effect detection function to the 'cleaned_text' column\n",
    "df['detected_side_effects'] = df['cleaned_text'].apply(lambda x: detect_side_effects(x, slang_lexicon, uncommon_vaccine_side_effects_lexicon))\n",
    "\n",
    "# Create binary flags for common and uncommon side effects\n",
    "df['has_common_side_effects'] = df['detected_side_effects'].apply(lambda x: any(effect in known_symptoms for effect in x))\n",
    "df['has_uncommon_side_effects'] = df['detected_side_effects'].apply(lambda x: any(effect in uncommon_vaccine_side_effects_lexicon for effect in x))\n",
    "\n",
    "# Correlate RT_Like with the detection of side effects (common and uncommon)\n",
    "common_side_effects_correlation = df.groupby('has_common_side_effects')['RT_Like'].mean()\n",
    "uncommon_side_effects_correlation = df.groupby('has_uncommon_side_effects')['RT_Like'].mean()\n",
    "\n",
    "# Visualization 1: Correlation between RT_Like and presence of common/uncommon side effects\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot common side effects correlation with RT_Like\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x=common_side_effects_correlation.index, y=common_side_effects_correlation.values, palette=\"Blues\")\n",
    "plt.title(\"RT_Like Correlation with Common Side Effects\")\n",
    "plt.xlabel(\"Common Side Effects Flag (0 = No, 1 = Yes)\")\n",
    "plt.ylabel(\"Average RT_Like\")\n",
    "\n",
    "# Plot uncommon side effects correlation with RT_Like\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x=uncommon_side_effects_correlation.index, y=uncommon_side_effects_correlation.values, palette=\"Oranges\")\n",
    "plt.title(\"RT_Like Correlation with Uncommon Side Effects\")\n",
    "plt.xlabel(\"Uncommon Side Effects Flag (0 = No, 1 = Yes)\")\n",
    "plt.ylabel(\"Average RT_Like\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import numpy as np\n",
    "from textblob import TextBlob  # Import TextBlob for sentiment analysis\n",
    "\n",
    "# Define Sample Undefined Variables\n",
    "severity_mapping = {\n",
    "    'headache': 1, 'sore arm': 1, 'dizziness': 1, 'nausea': 1, 'fatigue': 1, 'brain fog': 1,\n",
    "    'muscle aches': 1, 'chills': 1, 'feeling unwell': 1, 'fever': 2, 'muscle soreness': 2,\n",
    "    'joint pain': 2, 'shortness of breath': 2, 'extreme fatigue': 3, 'myocarditis': 3, 'stroke': 4, 'death': 5\n",
    "}\n",
    "\n",
    "# Function to categorize slang, time patterns, and side effects\n",
    "def categorize_slang_time_sideeffects(text):\n",
    "    matched_slangs = [slang for slang, symptoms in slang_lexicon.items() if any(s in text.lower() for s in symptoms)]\n",
    "    time_pattern_matches = [pattern for pattern in time_patterns if pattern in text.lower()]\n",
    "    side_effect_matches = [symptom for symptom in known_symptoms if symptom in text.lower()]\n",
    "    uncommon_effects_matches = [effect for effect in uncommon_vaccine_side_effects_lexicon if effect in text.lower()]\n",
    "    return matched_slangs, time_pattern_matches, side_effect_matches, uncommon_effects_matches\n",
    "\n",
    "# Apply categorization function to the 'Text0' column\n",
    "df[['slang_matches', 'time_pattern_matches', 'side_effect_matches', 'uncommon_effects_matches']] = df['Text0'].apply(lambda x: pd.Series(categorize_slang_time_sideeffects(x)))\n",
    "\n",
    "# Explode matches to separate rows\n",
    "df_exploded = df.explode('slang_matches').explode('time_pattern_matches').explode('side_effect_matches').explode('uncommon_effects_matches')\n",
    "\n",
    "# Sentiment analysis using TextBlob\n",
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    if polarity > 0:\n",
    "        return 'Positive'\n",
    "    elif polarity < 0:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Apply sentiment analysis to the side effects\n",
    "df_exploded['sentiment'] = df_exploded['side_effect_matches'].apply(lambda x: get_sentiment(x) if isinstance(x, str) else 'Neutral')\n",
    "\n",
    "# 1. Side Effect Frequency with Sentiment - Bar Chart\n",
    "side_effect_sentiment_counts = df_exploded.groupby(['side_effect_matches', 'sentiment']).size().reset_index(name='count')\n",
    "\n",
    "# Adjust plot size and label sizes\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=side_effect_sentiment_counts, x='side_effect_matches', y='count', hue='sentiment', palette='coolwarm')\n",
    "plt.xticks(rotation=90, fontsize=10)  # Reduce the size of x-axis labels\n",
    "plt.xlabel('Side Effect', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Side Effect Frequency with Sentiment', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Side Effect Severity Heatmap (without numbers)\n",
    "df_exploded['severity'] = df_exploded['side_effect_matches'].apply(lambda x: severity_mapping.get(x, np.nan))  # Use severity mapping to assign severity\n",
    "\n",
    "# Group by month and side effect, calculating average severity\n",
    "severity_data = df_exploded.groupby(['month', 'side_effect_matches'])['severity'].mean().unstack().fillna(np.nan)  # Ensure missing values are NaN\n",
    "\n",
    "# Plot heatmap without annotation (numbers)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(severity_data, cmap='YlGnBu', annot=False, cbar_kws={'label': 'Severity'}, linewidths=0.5, mask=severity_data.isna())  # Mask NaN values\n",
    "plt.title('Side Effect Severity Heatmap Across Months', fontsize=16)\n",
    "plt.xlabel('Side Effect', fontsize=12)\n",
    "plt.ylabel('Month', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Time Series of Top 10 Side Effect Mentions with Engagement (RT_Like) - Line Plot\n",
    "side_effect_mentions_by_month = df_exploded.groupby(['month', 'side_effect_matches']).size().reset_index(name='mention_count')\n",
    "engagement_by_month = df_exploded.groupby(['month', 'side_effect_matches'])['RT_Like'].sum().reset_index(name='total_RT_Like')\n",
    "\n",
    "# Merge the two datasets on 'month' and 'side_effect_matches'\n",
    "side_effect_time_series = pd.merge(side_effect_mentions_by_month, engagement_by_month, on=['month', 'side_effect_matches'])\n",
    "\n",
    "# Normalize Mention Counts and Engagements\n",
    "total_mentions = side_effect_time_series['mention_count'].sum()\n",
    "total_engagement = side_effect_time_series['total_RT_Like'].sum()\n",
    "\n",
    "side_effect_time_series['mention_normalized'] = side_effect_time_series['mention_count'] / total_mentions\n",
    "side_effect_time_series['engagement_normalized'] = side_effect_time_series['total_RT_Like'] / total_engagement\n",
    "\n",
    "# Compute a Weighted Score for Each Side Effect\n",
    "w_mention = 0.7  # Weight for mentions\n",
    "w_engagement = 0.3  # Weight for engagement\n",
    "\n",
    "# Calculate the weighted score\n",
    "side_effect_time_series['score'] = (w_mention * side_effect_time_series['mention_normalized']) + \\\n",
    "                                    (w_engagement * side_effect_time_series['engagement_normalized'])\n",
    "\n",
    "# Select Top 10 Side Effects Based on the Score\n",
    "top_10_side_effects = side_effect_time_series.groupby('side_effect_matches')['score'].max().reset_index()\n",
    "top_10_side_effects = top_10_side_effects.nlargest(10, 'score')\n",
    "\n",
    "# Filter the main dataset to include only the top 10 side effects\n",
    "top_10_side_effects_list = top_10_side_effects['side_effect_matches'].tolist()\n",
    "filtered_data = side_effect_time_series[side_effect_time_series['side_effect_matches'].isin(top_10_side_effects_list)]\n",
    "\n",
    "# Plot the Time Series with Mention Counts and Engagement\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.lineplot(data=filtered_data, x='month', y='mention_count', hue='side_effect_matches', marker='o', palette='Set2', lw=2)\n",
    "\n",
    "# Secondary Y-axis for Engagement (RT_Like)\n",
    "ax2 = plt.gca().twinx()\n",
    "sns.lineplot(data=filtered_data, x='month', y='total_RT_Like', hue='side_effect_matches', marker='x', ax=ax2, linestyle='--', palette='Set2', lw=2)\n",
    "\n",
    "ax2.set_ylabel('Total RT_Like', fontsize=12)  # Clarified label to represent engagement\n",
    "plt.title('Time Series of Top 10 Side Effect Mentions with Engagement (RT_Like)', fontsize=16)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "ax2.set_ylabel('Total RT_Like', fontsize=12)  # Corrected the right axis label\n",
    "\n",
    "# Adjusting the legends\n",
    "handles, labels = ax2.get_legend_handles_labels()\n",
    "side_effect_legend = plt.legend(handles, labels, title='Side Effect', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "mention_legend = plt.legend(title=\"Mentions\", loc='upper left', bbox_to_anchor=(1.05, 0.8))\n",
    "engagement_legend = plt.legend(title=\"Engagement (RT_Like)\", loc='upper left', bbox_to_anchor=(1.05, 0.6))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Extracting time pattern matches and slang lexicon matches\n",
    "def extract_time_pattern_matches(text, patterns):\n",
    "    matches = []\n",
    "    for pattern in patterns:\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            matches.append(pattern)\n",
    "    return matches\n",
    "\n",
    "def extract_slang_matches(text, slang_dict):\n",
    "    matches = []\n",
    "    for key, values in slang_dict.items():\n",
    "        for slang in values:\n",
    "            if slang in text:\n",
    "                matches.append(key)\n",
    "    return matches\n",
    "\n",
    "\n",
    "\n",
    "# Apply the functions to extract patterns and slang\n",
    "df['time_pattern_matches'] = df['Text0'].apply(lambda x: extract_time_pattern_matches(x, time_patterns))\n",
    "df['slang_matches'] = df['Text0'].apply(lambda x: extract_slang_matches(x, slang_lexicon))\n",
    "\n",
    "# Top 5 time patterns and slang lexicons by frequency\n",
    "time_pattern_counts = df['time_pattern_matches'].explode().value_counts().head(5)\n",
    "slang_counts = df['slang_matches'].explode().value_counts().head(5)\n",
    "\n",
    "# Example to show tweet volume for the top 5\n",
    "top_time_patterns = time_pattern_counts\n",
    "top_slangs = slang_counts\n",
    "\n",
    "# Prepare data for plotting\n",
    "time_pattern_stats_sorted = pd.DataFrame({\n",
    "    'time_pattern_matches': top_time_patterns.index,\n",
    "    'tweet_volume': top_time_patterns.values\n",
    "})\n",
    "\n",
    "slang_lexicon_stats_sorted = pd.DataFrame({\n",
    "    'slang_matches': top_slangs.index,\n",
    "    'tweet_volume': top_slangs.values\n",
    "})\n",
    "\n",
    "# Calculate the average RT_Like for each time pattern and slang lexicon\n",
    "avg_rt_likes_time_patterns = [\n",
    "    df[df['time_pattern_matches'].apply(lambda x: pattern in x)]['RT_Like'].mean()\n",
    "    for pattern in top_time_patterns.index\n",
    "]\n",
    "\n",
    "avg_rt_likes_slangs = [\n",
    "    df[df['slang_matches'].apply(lambda x: slang in x)]['RT_Like'].mean()\n",
    "    for slang in top_slangs.index\n",
    "]\n",
    "\n",
    "# Combine the average RT_Likes for both time patterns and slang lexicons\n",
    "avg_rt_likes = avg_rt_likes_time_patterns + avg_rt_likes_slangs\n",
    "\n",
    "# Calculate the sentiment polarity for each tweet and compute the average sentiment for each pattern/lexicon\n",
    "df['sentiment'] = df['Text0'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Now calculate the average sentiment for the top 5 time patterns and slang lexicons\n",
    "avg_sentiment_time_patterns = [\n",
    "    df[df['time_pattern_matches'].apply(lambda x: pattern in x)].get('sentiment', pd.Series(dtype='float64')).mean()\n",
    "    for pattern in top_time_patterns.index\n",
    "]\n",
    "avg_sentiment_slangs = [\n",
    "    df[df['slang_matches'].apply(lambda x: slang in x)].get('sentiment', pd.Series(dtype='float64')).mean()\n",
    "    for slang in top_slangs.index\n",
    "]\n",
    "\n",
    "# Combine the sentiment values with the existing data\n",
    "avg_sentiments = avg_sentiment_time_patterns + avg_sentiment_slangs\n",
    "\n",
    "# Prepare x labels and tweet volumes for plotting\n",
    "x_labels = list(top_time_patterns.index)[:5] + list(top_slangs.index)[:5]\n",
    "tweet_volumes = list(time_pattern_stats_sorted['tweet_volume'])[:5] + list(slang_lexicon_stats_sorted['tweet_volume'])[:5]\n",
    "\n",
    "# Create the plot\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Create the bar plot for tweet volume with different colors for time patterns and slang lexicons\n",
    "sns.barplot(x=x_labels[:5], y=tweet_volumes[:5], color='skyblue', label='Time Patterns', ax=ax1)\n",
    "sns.barplot(x=x_labels[5:], y=tweet_volumes[5:], color='orange', label='Slang Lexicons', ax=ax1)\n",
    "\n",
    "# Create the second y-axis for sentiment\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(x_labels, avg_sentiments, color='green', label='Average Sentiment', marker='o', linestyle='--')\n",
    "\n",
    "# Create a third y-axis for average RT_Likes\n",
    "ax3 = ax1.twinx()\n",
    "ax3.spines['right'].set_position(('outward', 60))  # Shift the third axis to the right\n",
    "ax3.plot(x_labels, avg_rt_likes, color='red', label='Average RT_Likes', marker='x', linestyle=':')\n",
    "\n",
    "# Set x-ticks and labels\n",
    "xticks = list(range(len(x_labels)))  # Generate tick positions based on the number of x_labels\n",
    "ax1.set_xticks(xticks)  # Set the positions for the x-ticks\n",
    "ax1.set_xticklabels([label[:15] for label in x_labels], rotation=90)  # Set the labels with a limit of 15 characters\n",
    "\n",
    "# Add titles and labels\n",
    "ax1.set_title('Tweet Volume, Average RT_Likes, and Average Sentiment for Top 5 Time Patterns and Slang Lexicons', fontsize=14)\n",
    "ax1.set_xlabel('Time Patterns and Slang Lexicons', fontsize=12)\n",
    "ax1.set_ylabel('Tweet Volume', fontsize=12)\n",
    "ax2.set_ylabel('Average Sentiment', fontsize=12)\n",
    "ax3.set_ylabel('Average RT_Likes', fontsize=12)\n",
    "\n",
    "# Display the legend\n",
    "ax1.legend(title=\"Categories\", loc='upper left')\n",
    "ax2.legend(title=\"Sentiment\", loc='upper right')\n",
    "ax3.legend(title=\"Average RT_Likes\", loc='lower right')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Load spaCy's medium-sized English model with word vectors\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Function to normalize repeated characters (e.g., \"Feeeeeeeeeeel\" -> \"Feel\")\n",
    "def normalize_repeated_characters(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "# Function to compute the average word vector for a given text using spaCy\n",
    "def get_average_word2vec(text, model):\n",
    "    doc = model(text)\n",
    "    word_vectors = [token.vector for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "    if len(word_vectors) == 0:\n",
    "        return None  # Return None if no valid word vectors are found\n",
    "    else:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Function to match patterns from any lexicon (time or slang)\n",
    "def match_patterns(text_string, lexicon, similarity_threshold=0.5):\n",
    "    match_data = []  # To store matching results\n",
    "    text_string = normalize_repeated_characters(text_string)\n",
    "\n",
    "    # Iterate over each pattern in the lexicon\n",
    "    for pattern, label in lexicon.items():\n",
    "        matches = re.findall(pattern, text_string, flags=re.IGNORECASE)  # Case-insensitive matching\n",
    "        \n",
    "        if matches:\n",
    "            for match in matches:\n",
    "                match_data.append({\n",
    "                    'Matched Expression': match,\n",
    "                    'Label': label,\n",
    "                    'Similarity Score': 'N/A',\n",
    "                    'Match Type': 'Direct Match'\n",
    "                })\n",
    "                \n",
    "    return match_data\n",
    "\n",
    "# Function to perform fuzzy matching using cosine similarity for lexicons\n",
    "def fuzzy_match_lexicon_patterns(text_string, lexicons, similarity_threshold=0.5):\n",
    "    match_data = []\n",
    "    for lexicon_name, lexicon in lexicons.items():\n",
    "        for pattern, label in lexicon.items():\n",
    "            normalized_pattern = normalize_repeated_characters(pattern)\n",
    "            pattern_vec = get_average_word2vec(normalized_pattern, nlp)\n",
    "            \n",
    "            if pattern_vec is not None:\n",
    "                input_vec = get_average_word2vec(text_string, nlp)\n",
    "                \n",
    "                if input_vec is not None:\n",
    "                    similarity_score = cosine_similarity([pattern_vec], [input_vec])[0][0]\n",
    "                    \n",
    "                    if similarity_score >= similarity_threshold:\n",
    "                        match_data.append({\n",
    "                            'Matched Expression': normalized_pattern,\n",
    "                            'Label': label,\n",
    "                            'Similarity Score': similarity_score,\n",
    "                            'Match Type': 'Fuzzy Match (cosine similarity)'\n",
    "                        })\n",
    "    \n",
    "    return match_data\n",
    "\n",
    "# Combine the direct matching and fuzzy matching\n",
    "def process_text(text_string, time_pattern_labels, slang_lexicon, similarity_threshold=0.5):\n",
    "    # Perform direct regex-based matching\n",
    "    direct_matches = match_patterns(text_string, time_pattern_labels, similarity_threshold)\n",
    "    direct_matches += match_patterns(text_string, slang_lexicon, similarity_threshold)\n",
    "\n",
    "    # Perform fuzzy matching using spaCy vectors\n",
    "    fuzzy_matches_time = fuzzy_match_lexicon_patterns(text_string, {'Time Patterns': time_pattern_labels}, similarity_threshold)\n",
    "    fuzzy_matches_slang = fuzzy_match_lexicon_patterns(text_string, {'Slang': slang_lexicon}, similarity_threshold)\n",
    "\n",
    "    # Combine all match data\n",
    "    all_matches = direct_matches + fuzzy_matches_time + fuzzy_matches_slang\n",
    "    \n",
    "    if all_matches:\n",
    "        # Create DataFrame for better visualization\n",
    "        match_df = pd.DataFrame(all_matches)\n",
    "        \n",
    "        # Sort results by similarity score in descending order and reset index\n",
    "        match_df = match_df.sort_values(by='Similarity Score', ascending=False, na_position='last').reset_index(drop=True)\n",
    "        \n",
    "        # Format the output for better display\n",
    "        match_df['Similarity Score'] = match_df['Similarity Score'].apply(lambda x: f\"{x:.6f}\" if isinstance(x, float) else x)\n",
    "        \n",
    "        # Truncate or shorten long labels for better readability\n",
    "        match_df['Label'] = match_df['Label'].apply(lambda x: f\"{str(x)[:40]}...\" if len(str(x)) > 40 else x)\n",
    "        match_df['Matched Expression'] = match_df['Matched Expression'].apply(lambda x: f\"{str(x)[:30]}...\" if len(str(x)) > 30 else x)\n",
    "        \n",
    "        return match_df\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Sample input text (this would be input from the user)\n",
    "text_string = input(\"Enter your text for processing: \")\n",
    "\n",
    "# Process the input text (ensure you define `time_pattern_labels` and `slang_lexicon` beforehand)\n",
    "matches_df = process_text(text_string, time_pattern_labels, slang_lexicon, similarity_threshold=0.5)\n",
    "\n",
    "# Display the results in a tabular format using tabulate\n",
    "if matches_df is not None and not matches_df.empty:\n",
    "    print(\"\\nMatched Expressions:\")\n",
    "    print(tabulate(matches_df, headers='keys', tablefmt='grid', showindex=False))\n",
    "else:\n",
    "    print(\"No matches found above the similarity threshold.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb195e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fd9e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a78dda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338197c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
